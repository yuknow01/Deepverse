{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20cdba8",
   "metadata": {},
   "source": [
    "#  Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6cc1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello worlds\n"
     ]
    }
   ],
   "source": [
    "print(\"hello worlds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0c35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as  np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader, Subset\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from deepverse import ParameterManager\n",
    "from deepverse.scenario import ScenarioManager\n",
    "from deepverse import Dataset\n",
    "\n",
    "from deepverse.visualizers import ImageVisualizer, LidarVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babd3da",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7df4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenes 100\n",
    "## Subcarriers 64\n",
    "\n",
    "scenarios_name = \"DT31\"\n",
    "config_path = f\"scenarios/{scenarios_name}/param/config.m\"\n",
    "param_manager = ParameterManager(config_path)\n",
    "\n",
    "params = param_manager.get_params()\n",
    "\n",
    "param_manager.params[\"scenes\"] =list(range(100))\n",
    "param_manager.params[\"comm\"][\"OFDM\"][\"selected_subcarriers\"] = list(range(64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d7ca8",
   "metadata": {},
   "source": [
    "# Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d341a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating camera dataset: ⏳ In progress\n",
      "\u001b[F\u001b[KGenerating camera dataset: ✅ Completed (0.02s)\n",
      "Generating LiDAR dataset: ⏳ In progress\n",
      "\u001b[F\u001b[KGenerating LiDAR dataset: ✅ Completed (0.00s)\n",
      "Generating mobility dataset: ⏳ In progress\n",
      "\u001b[F\u001b[KGenerating mobility dataset: ✅ Completed (0.00s)\n",
      "Generating comm dataset: ⏳ In progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F\u001b[KGenerating comm dataset: ✅ Completed (4.86s)\n",
      "Generating radar dataset: ⏳ In progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F\u001b[KGenerating radar dataset: ✅ Completed (236.69s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(param_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0fe9a",
   "metadata": {},
   "source": [
    "# location dataset \n",
    " 지금 실험에서는 안쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db8cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm = dataset.comm_dataset\n",
    "# location =  comm\n",
    "\n",
    "# location = [\n",
    "#     {\n",
    "#         \"bs_loc\": d[\"bs_loc\"],                      # (3,)\n",
    "#         \"ue_loc\": np.asarray(d[\"ue_loc\"]).squeeze() # (3,)  (원래 (1,3)이면 squeeze)\n",
    "#     }\n",
    "#     for row in comm.data      # row: [dict] 형태\n",
    "#     for d in row              # d: dict\n",
    "# ]\n",
    "\n",
    "# print(ue_location)  #)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4fe58",
   "metadata": {},
   "source": [
    "# communication  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715162f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "# UE 정보\n",
    "comm = dataset.comm_dataset\n",
    "ch = comm.data[0][0]['ue'][0]\n",
    "print(ch.coeffs.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0f7be",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e21952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeffs_from_frame(frame, ue_idx=0):\n",
    "    ue_obj = frame[\"ue\"]\n",
    "\n",
    "    # 케이스1) list/tuple이면 ue_idx로 선택\n",
    "    if isinstance(ue_obj, (list, tuple)):\n",
    "        ch_obj = ue_obj[ue_idx]\n",
    "    else:\n",
    "        # 케이스2) 단일 OFDMChannel이면 그대로 사용\n",
    "        ch_obj = ue_obj\n",
    "\n",
    "    # coeffs는 dict key가 아니라 attribute일 확률이 매우 큼\n",
    "    if hasattr(ch_obj, \"coeffs\"):\n",
    "        return ch_obj.coeffs\n",
    "\n",
    "    # 혹시 dict라면 마지막 보험\n",
    "    if isinstance(ch_obj, dict) and \"coeffs\" in ch_obj:\n",
    "        return ch_obj[\"coeffs\"]\n",
    "\n",
    "    raise TypeError(f\"Cannot get coeffs. ue type={type(ue_obj)}, ch type={type(ch_obj)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cce7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_min_max_realimag(frames, train_idx, us_idx=0):\n",
    "\n",
    "    rmin, rmax =  float('inf'), float('-inf')\n",
    "    imin, imax =  float('inf'), float('-inf')\n",
    "\n",
    "    print(\"Calculating min/max over training set...\")\n",
    "\n",
    "    for t  in train_idx:\n",
    "        frame  = frames[t]\n",
    "        cooeffs  = get_coeffs_from_frame(frame, us_idx)  # (N_subcarriers, )\n",
    "\n",
    "        rmin = min(rmin, float(cooeffs.real.min()))\n",
    "        rmax = max(rmax, float(cooeffs.real.max()))\n",
    "        imin = min(imin, float(cooeffs.imag.min()))\n",
    "        imax = max(imax, float(cooeffs.imag.max()))\n",
    "\n",
    "    print(f\"Done. rmin={rmin}, rmax={rmax}, imin={imin}, imax={imax}\")\n",
    "    return (rmin, rmax), (imin, imax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cb0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_channel_coeffs_minmax(coeffs_np, r_min, r_max, i_min, i_max, device=\"cuda\", eps=1e-12):\n",
    "    # Convert Numpy to Tensor\n",
    "    coeffs = torch.from_numpy(coeffs_np).to(torch.complex64)\n",
    "    \n",
    "    r = coeffs.real\n",
    "    i = coeffs.imag\n",
    "    \n",
    "    # Min-Max Scaling [0, 1]\n",
    "    # Add eps to denominator to prevent division by zero\n",
    "    r_scaled = (r - r_min) / max(r_max - r_min, eps)\n",
    "    i_scaled = (i - i_min) / max(i_max - i_min, eps)\n",
    "    \n",
    "    # Concat (Maintains shape like (..., 2*subcarriers))\n",
    "    H = torch.cat([r_scaled, i_scaled], dim=-1).to(device)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "639969a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "# 사용예시\n",
    "H = preprocess_channel_coeffs_minmax(ch.coeffs, r_min=-0.5, r_max=0.5, i_min=-0.5, i_max=0.5)\n",
    "print(H.shape)  # (1, 16, 128) 64 subcar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f94e",
   "metadata": {},
   "source": [
    "### image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ef4385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: scenarios/DT31/RGB_images/unit1_cam1/0.png\n",
      "PIL size (W,H): (1920, 1080)\n",
      "np shape: (1080, 1920, 3) dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "sensor = dataset.camera_dataset.sensors[\"unit1_cam1\"]\n",
    "path0 = sensor.files[0]\n",
    "img = Image.open(path0).convert(\"RGB\")\n",
    "arr = np.array(img)\n",
    "\n",
    "print(\"path:\", path0)\n",
    "print(\"PIL size (W,H):\", img.size)\n",
    "print(\"np shape:\", arr.shape, \"dtype:\", arr.dtype)  # 보통 (H,W,3), uint8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b56a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def preprocess_img(path, img_size=IMG_SIZE, device=\"cuda\"):\n",
    "    # 1) load (H,W,3) uint8\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "\n",
    "    # 2) numpy -> torch, (3,H,W), float32\n",
    "    x = torch.from_numpy(arr).permute(2, 0, 1).contiguous().float()\n",
    "    x = x / 255.0  # [0,1]\n",
    "\n",
    "    # 3) add batch dim -> (1,3,H,W)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # 4) resize -> (1,3,224,224)\n",
    "    x = F.interpolate(x, size=(img_size, img_size),\n",
    "                      mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # 5) normalize (ImageNet)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], dtype=x.dtype).view(1, 3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], dtype=x.dtype).view(1, 3, 1, 1)\n",
    "    x = (x - mean) / std\n",
    "\n",
    "    # 6) move to device (GPU)\n",
    "    x = x.to(device, non_blocking=True)\n",
    "\n",
    "    return x  # (1,3,224,224) on device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfa24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) cuda:0\n",
      "scenarios/DT31/RGB_images/unit1_cam1/0.png\n"
     ]
    }
   ],
   "source": [
    "# 사용예시\n",
    "cd = dataset.camera_dataset\n",
    "sensor = cd.sensors['unit1_cam1']\n",
    "path0 = sensor.files[0]\n",
    "img = preprocess_img(path0, device=\"cuda\")\n",
    "print(img.shape, img.device)  # torch.Size([1,3,224,224]) cuda:0\n",
    "print(path0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a4765",
   "metadata": {},
   "source": [
    "# Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faab47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comm_frames(comm):\n",
    "    frames = []\n",
    "    for row in comm.data:\n",
    "        for d in row:\n",
    "            frames.append(d)\n",
    "    return frames\n",
    "\n",
    "class MultiModalNextStepDatasetGPU(TorchDataset):\n",
    "    def __init__(self, comm_frames, cam_files, ue_idx=0, past_len=15, device=\"cuda\",\n",
    "                 # Arguments for statistical values (initialized with default values)\n",
    "                 r_min=0.0, r_max=1.0, i_min=0.0, i_max=1.0):\n",
    "        \n",
    "        self.comm_frames = comm_frames\n",
    "        self.cam_files = list(cam_files)\n",
    "        self.ue_idx = ue_idx\n",
    "        self.past_len = past_len\n",
    "        self.device = device\n",
    "        \n",
    "        # Save statistical values\n",
    "        self.r_min, self.r_max = r_min, r_max\n",
    "        self.i_min, self.i_max = i_min, i_max\n",
    "\n",
    "        self.N = min(len(self.comm_frames), len(self.cam_files))\n",
    "        self.valid_start = past_len - 1\n",
    "        self.valid_end = self.N - 2 \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.valid_end - self.valid_start + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.valid_start + idx\n",
    "\n",
    "        # 1. Image Past (Apply Preprocessing)\n",
    "        img_list = []\n",
    "        for k in range(t - self.past_len + 1, t + 1):\n",
    "            img_path = self.cam_files[k]\n",
    "            img_k = preprocess_img(img_path, device=self.device).squeeze(0)\n",
    "            img_list.append(img_k)\n",
    "        img = torch.stack(img_list, dim=0)  # Shape: (past_len\n",
    "\n",
    "        \n",
    "        # 2. Channel Past (Apply Scaling)\n",
    "        ch_list = []\n",
    "        for k in range(t - self.past_len + 1, t + 1):\n",
    "            coeffs_np = get_coeffs_from_frame(self.comm_frames[k], ue_idx=self.ue_idx)\n",
    "            # Use the newly defined Min-Max preprocessing function\n",
    "            h = preprocess_channel_coeffs_minmax(\n",
    "                coeffs_np, \n",
    "                self.r_min, self.r_max, self.i_min, self.i_max, \n",
    "                device=self.device\n",
    "            ).reshape(-1)\n",
    "            ch_list.append(h)\n",
    "        channel_past = torch.stack(ch_list, dim=0)\n",
    "\n",
    "        # 3. Target (Apply Scaling) - Target must also be scaled for model training!\n",
    "        coeffs_np_next = get_coeffs_from_frame(self.comm_frames[t + 1], ue_idx=self.ue_idx)\n",
    "        target = preprocess_channel_coeffs_minmax(\n",
    "            coeffs_np_next, \n",
    "            self.r_min, self.r_max, self.i_min, self.i_max, \n",
    "            device=self.device\n",
    "        ).reshape(-1)\n",
    "\n",
    "        return channel_past, img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24ae3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "629d15c1",
   "metadata": {},
   "source": [
    "# DataLoader 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c940a7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 2048]) torch.Size([8, 16, 3, 224, 224]) torch.Size([8, 2048])\n",
      "cuda:0 cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "comm_frames = flatten_comm_frames(dataset.comm_dataset)\n",
    "sensor = dataset.camera_dataset.sensors[\"unit1_cam1\"]\n",
    "\n",
    "ds = MultiModalNextStepDatasetGPU(\n",
    "    comm_frames=comm_frames,\n",
    "    cam_files=sensor.files,\n",
    "    ue_idx=0,\n",
    "    past_len=16,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,     \n",
    "    pin_memory=False   # ✅ 의미 없음 (이미 GPU)\n",
    ")\n",
    "\n",
    "ch, img, y = next(iter(loader))\n",
    "print(ch.shape, img.shape, y.shape)\n",
    "print(ch.device, img.device, y.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5cac5",
   "metadata": {},
   "source": [
    "# ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2100781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# 1) ConvLSTM building blocks\n",
    "# -----------------------------\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_channels + hidden_channels,\n",
    "            out_channels=4 * hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        # x: (B, Cin, H, W)\n",
    "        # state: (h, c) each (B, Chid, H, W)\n",
    "        h, c = state\n",
    "        combined = torch.cat([x, h], dim=1)  # (B, Cin+Chid, H, W)\n",
    "        gates = self.conv(combined)          # (B, 4*Chid, H, W)\n",
    "\n",
    "        i, f, o, g = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  x (B,T,C,H,W)\n",
    "    Output: last_h (B,hidden,H,W), (last_h,last_c)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_channels=64, num_layers=2, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        cells = []\n",
    "        for layer in range(num_layers):\n",
    "            cin = input_channels if layer == 0 else hidden_channels\n",
    "            cells.append(ConvLSTMCell(cin, hidden_channels, kernel_size=kernel_size))\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C,H,W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # init states\n",
    "        hs, cs = [], []\n",
    "        for _ in range(self.num_layers):\n",
    "            hs.append(torch.zeros(B, self.hidden_channels, H, W, device=device, dtype=x.dtype))\n",
    "            cs.append(torch.zeros(B, self.hidden_channels, H, W, device=device, dtype=x.dtype))\n",
    "\n",
    "        # time loop\n",
    "        for t in range(T):\n",
    "            inp = x[:, t]  # (B,C,H,W)\n",
    "            for l, cell in enumerate(self.cells):\n",
    "                h, c = hs[l], cs[l]\n",
    "                h, c = cell(inp, (h, c))\n",
    "                hs[l], cs[l] = h, c\n",
    "                inp = h\n",
    "                if self.dropout > 0 and l < self.num_layers - 1:\n",
    "                    inp = F.dropout(inp, p=self.dropout, training=self.training)\n",
    "\n",
    "        return hs[-1], (hs[-1], cs[-1])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Encoders: image->map, channel->map\n",
    "# -----------------------------\n",
    "class ImgFrameEncoderMap(nn.Module):\n",
    "    \"\"\"\n",
    "    (B*T,3,224,224) -> (B*T,Cm,Hm,Wm)  (Hm,Wm fixed by adaptive pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=64, out_hw=14):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=2, padding=2), nn.ReLU(),   # 224 -> 112\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),  # 112 -> 56\n",
    "            nn.Conv2d(64, out_channels, 3, stride=2, padding=1), nn.ReLU(),  # 56 -> 28\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((out_hw, out_hw))  # 28 -> out_hw\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.backbone(x)\n",
    "        z = self.pool(z)\n",
    "        return z  # (B*T, out_channels, out_hw, out_hw)\n",
    "\n",
    "\n",
    "class ChannelVecToMap(nn.Module):\n",
    "    \"\"\"\n",
    "    (B,T,F_in) -> (B,T,Cc,Hm,Wm)\n",
    "    \"\"\"\n",
    "    def __init__(self, F_in, Cc=16, out_hw=14, hidden=512):\n",
    "        super().__init__()\n",
    "        self.Cc = Cc\n",
    "        self.out_hw = out_hw\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(F_in, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, Cc * out_hw * out_hw),\n",
    "        )\n",
    "\n",
    "    def forward(self, ch):\n",
    "        B, T, F = ch.shape\n",
    "        z = self.proj(ch)  # (B,T,Cc*H*W)\n",
    "        z = z.view(B, T, self.Cc, self.out_hw, self.out_hw)\n",
    "        return z\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) ConvLSTM Early Fusion Forecaster\n",
    "# -----------------------------\n",
    "class ConvLSTM_EarlyFusion_Forecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    ch : (B,T,F_in=2048)\n",
    "    img: (B,T,3,224,224)\n",
    "    yhat: (B,F_out)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        F_in,\n",
    "        F_out,\n",
    "        img_map_ch=64,\n",
    "        ch_map_ch=16,\n",
    "        map_hw=14,\n",
    "        convlstm_hidden=64,\n",
    "        convlstm_layers=2,\n",
    "        convlstm_kernel=3,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.map_hw = map_hw\n",
    "        self.img_enc = ImgFrameEncoderMap(out_channels=img_map_ch, out_hw=map_hw)\n",
    "        self.ch_map = ChannelVecToMap(F_in=F_in, Cc=ch_map_ch, out_hw=map_hw)\n",
    "\n",
    "        fused_c = img_map_ch + ch_map_ch\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_channels=fused_c,\n",
    "            hidden_channels=convlstm_hidden,\n",
    "            num_layers=convlstm_layers,\n",
    "            kernel_size=convlstm_kernel,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # (B,Ch,1,1)\n",
    "            nn.Flatten(),                  # (B,Ch)\n",
    "            nn.Linear(convlstm_hidden, F_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, ch, img):\n",
    "        # ch:  (B,T,F_in)\n",
    "        # img: (B,T,3,224,224)\n",
    "        B, T = ch.shape[0], ch.shape[1]\n",
    "\n",
    "        # image -> map\n",
    "        img_ = img.view(B * T, *img.shape[2:])      # (B*T,3,224,224)\n",
    "        img_map = self.img_enc(img_)                # (B*T,Cm,Hm,Wm)\n",
    "        img_map = img_map.view(B, T, *img_map.shape[1:])  # (B,T,Cm,Hm,Wm)\n",
    "\n",
    "        # channel -> map\n",
    "        ch_map = self.ch_map(ch)                    # (B,T,Cc,Hm,Wm)\n",
    "\n",
    "        # early fusion on maps\n",
    "        x = torch.cat([img_map, ch_map], dim=2)     # (B,T,Cm+Cc,Hm,Wm)\n",
    "\n",
    "        # ConvLSTM over time\n",
    "        last_h, _ = self.convlstm(x)                # (B, hidden, Hm, Wm)\n",
    "\n",
    "        # head -> (B,F_out)\n",
    "        yhat = self.head(last_h)\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2511a",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "data shape 맞추기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c529263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from lwm_multi_model import multi_modal_lwm  # 너가 올린 backbone\n",
    "\n",
    "# class FinetuneChannelPredictor(nn.Module):\n",
    "#     \"\"\"\n",
    "#     직접 구현한 lwm_multi_model(channel + image)에 맞는 파인튜닝 모델\n",
    "#     Input:\n",
    "#       ch:  (B, T, F_in)  e.g., (B,16,2048) 16: past Length, 2048: feature dim\n",
    "#       img: (B, 3, 224, 224)\n",
    "#     Output:\n",
    "#       yhat: (B, F_out)  e.g., (B,2048)\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         backbone: nn.Module,\n",
    "#         F_in: int,\n",
    "#         F_out: int,\n",
    "#         pool: str = \"last\",          # \"last\" or \"mean\"\n",
    "#         freeze_image: bool = False,\n",
    "#         freeze_backbone: bool = False,\n",
    "#         element_length: int = 16,    # 채널 벡터 차원 (backbone 기대값)\n",
    "#         d_model: int = 64            # backbone 내부 feature dim\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.pool = pool\n",
    "\n",
    "#         # backbone이 기대하는 channel feature dim = ELEMENT_LENGTH\n",
    "#         # (backbone 내부 Channel_Embedding: Linear(ELEMENT_LENGTH -> D_MODEL))\n",
    "#         if element_length is None:\n",
    "#             element_length = backbone.channel_embedding.element_length\n",
    "#         if d_model is None:\n",
    "#             d_model = backbone.channel_embedding.d_model\n",
    "\n",
    "#         # 입력 차원 정렬: F_in -> ELEMENT_LENGTH\n",
    "#         self.in_proj = nn.Sequential(\n",
    "#             nn.Linear(F_in, 512),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(512, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(128, element_length)\n",
    "#         )\n",
    "\n",
    "#         # 출력 head: D_MODEL -> F_out\n",
    "#         self.head = nn.Linear(d_model, F_out)\n",
    "\n",
    "#         if freeze_image:\n",
    "#             for p in self.backbone.image_embedding.parameters():\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "#         if freeze_backbone:\n",
    "#             for p in self.backbone.parameters():\n",
    "#                 p.requires_grad = False\n",
    "#             # 그래도 projection/head는 학습되게 다시 켜기\n",
    "#             for p in self.in_proj.parameters():\n",
    "#                 p.requires_grad = True\n",
    "#             for p in self.head.parameters():\n",
    "#                 p.requires_grad = True\n",
    "\n",
    "#     def forward(self, ch, img):\n",
    "#         # ch: (B,T,F_in) -> (B,T,ELEMENT_LENGTH)\n",
    "#         ch = self.in_proj(ch)\n",
    "\n",
    "#         # backbone: (B,T,D_MODEL)/\n",
    "#         tokens = self.backbone(ch, img)\n",
    "\n",
    "#         # pooling -> (B,D_MODEL)\n",
    "#         if self.pool == \"last\":\n",
    "#             z = tokens[:, -1, :]\n",
    "#         elif self.pool == \"mean\":\n",
    "#             z = tokens.mean(dim=1)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown pool={self.pool}\")\n",
    "\n",
    "#         # head -> (B,F_out)\n",
    "#         yhat = self.head(z)\n",
    "#         return yhat\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c58c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66a391b0",
   "metadata": {},
   "source": [
    "## NMSE(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c804d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def nmse_db(yhat: torch.Tensor, y: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    # yhat, y: (B,F)\n",
    "    num = torch.sum((yhat - y) ** 2, dim=1)\n",
    "    den = torch.sum(y ** 2, dim=1).clamp_min(eps)\n",
    "    nmse = num / den\n",
    "    return 10.0 * torch.log10(nmse.clamp_min(eps)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdc594",
   "metadata": {},
   "source": [
    "# Train/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba34747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating min/max over training set...\n",
      "Done. rmin=-1.0646139668415024e-06, rmax=1.091798991441695e-06, imin=-1.0719515918791155e-06, imax=1.0643867764302909e-06\n",
      "Dataset statistical values set in the dataset.\n",
      "\n",
      "=== Data Check ===\n",
      "y stats | min: 0.0000, max: 1.0000\n",
      "If scaling worked correctly, values should be within [0, 1].\n"
     ]
    }
   ],
   "source": [
    "n = len(ds)\n",
    "n_train = int(0.75 * n)\n",
    "train_idx = list(range(0, n_train))\n",
    "val_idx = list(range(n_train, n))\n",
    "\n",
    "train_ts = [ds.valid_start + i for i in train_idx]\n",
    "\n",
    "(real_min,  real_max), (imag_min, imag_max) = get_train_min_max_realimag(\n",
    "    comm_frames, train_ts, us_idx=0\n",
    ")\n",
    "\n",
    "ds.r_min = real_min\n",
    "ds.r_max = real_max\n",
    "ds.i_min = imag_min\n",
    "ds.i_max = imag_max\n",
    "\n",
    "print(\"Dataset statistical values set in the dataset.\")\n",
    "\n",
    "train_ds = Subset(ds, train_idx)\n",
    "val_ds   = Subset(ds, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0)\n",
    "F_in = 2048  # \n",
    "\n",
    "# Verify\n",
    "ch, img, y = next(iter(train_loader))\n",
    "F_out = y.shape[-1]\n",
    "print(\"\\n=== Data Check ===\")\n",
    "print(f\"y stats | min: {y.min().item():.4f}, max: {y.max().item():.4f}\")\n",
    "print(\"If scaling worked correctly, values should be within [0, 1].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dccdd060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(comm_frames): 100\n",
      "len(cam_files): 7012\n",
      "first comm frame keys: ['bs_loc', 'ue', 'ue_loc', 'bs']\n",
      "first cam file: scenarios/DT31/RGB_images/unit1_cam1/0.png\n"
     ]
    }
   ],
   "source": [
    "comm_frames = flatten_comm_frames(dataset.comm_dataset)\n",
    "cam_files = list(dataset.camera_dataset.sensors[\"unit1_cam1\"].files)\n",
    "\n",
    "print(\"len(comm_frames):\", len(comm_frames))\n",
    "print(\"len(cam_files):\", len(cam_files))\n",
    "print(\"first comm frame keys:\", list(comm_frames[0].keys()))\n",
    "print(\"first cam file:\", cam_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa662c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbf8fb",
   "metadata": {},
   "source": [
    "# Model generate and also check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "692cc0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device:\", device)\n",
    "\n",
    "# # 배치 하나로 F_in/F_out 자동 확정\n",
    "# ch, img, y = next(iter(train_loader))\n",
    "# F_in  = ch.shape[-1]\n",
    "# F_out = y.shape[-1]\n",
    "# print(\"Detected:\", \"F_in=\", F_in, \"F_out=\", F_out)\n",
    "# print(\"Batch devices:\", ch.device, img.device, y.device)\n",
    "\n",
    "# # backbone + finetune model\n",
    "# backbone = multi_modal_lwm().to(device)\n",
    "\n",
    "# model = FinetuneChannelPredictor(\n",
    "#     backbone=backbone,\n",
    "#     F_in=F_in,\n",
    "#     F_out=F_out,\n",
    "#     pool=\"last\",            # \"mean\"으로 바꿔도 됨\n",
    "#     freeze_image=False,     # 원하면 True (이미지 인코더 고정)\n",
    "#     freeze_backbone=False,  # 원하면 True (proj/head만 학습)\n",
    "#     element_length=16,\n",
    "#     d_model=64\n",
    "# ).to(device)\n",
    "\n",
    "# # sanity forward\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # ds가 이미 cuda 텐서 반환이면 아래 .to(device) 생략 가능\n",
    "#     yhat = model(ch.to(device), img.to(device))\n",
    "# print(\"yhat:\", yhat.shape, \"y:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5893214",
   "metadata": {},
   "source": [
    "# Train/ Eval 함수 (AMP + grad clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb7f6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train_one_epoch(model, loader, optimizer, device, use_amp=True, grad_clip=1.0):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_nmse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for ch, img, y in loader:\n",
    "        # Dataset이 이미 cuda 텐서를 반환하더라도 안전하게 유지\n",
    "        ch = ch.to(device, non_blocking=True)\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        y  = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            yhat = model(ch, img)\n",
    "            loss = F.mse_loss(yhat, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if grad_clip is not None and grad_clip > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_nmse += nmse_db(yhat.detach(), y).item()\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(n, 1), total_nmse / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_nmse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for ch, img, y in loader:\n",
    "        ch = ch.to(device, non_blocking=True)\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        y  = y.to(device, non_blocking=True)\n",
    "\n",
    "        yhat = model(ch, img)\n",
    "        loss = F.mse_loss(yhat, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_nmse += nmse_db(yhat, y).item()\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(n, 1), total_nmse / max(n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ee4deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def fit_model(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs: int,\n",
    "    ckpt_path: str,\n",
    "    lr: float = 1e-4,\n",
    "    weight_decay: float = 1e-4,\n",
    "    use_amp: bool = True,\n",
    "    grad_clip: float = 1.0,\n",
    "):\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(\"trainable params:\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # best trackers\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_nmse = float(\"inf\")  # (dB) lower is better\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        tr_loss, tr_nmse = train_one_epoch(\n",
    "            model, train_loader, optimizer,\n",
    "            device=device, use_amp=use_amp, grad_clip=grad_clip\n",
    "        )\n",
    "        va_loss, va_nmse = evaluate(model, val_loader, device=device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(\n",
    "            f\"[{epoch:02d}/{epochs}] \"\n",
    "            f\"train loss={tr_loss:.6f}, nmse(dB)={tr_nmse:.4f} | \"\n",
    "            f\"val loss={va_loss:.6f}, nmse(dB)={va_nmse:.4f} | \"\n",
    "            f\"{dt:.1f}s\"\n",
    "        )\n",
    "\n",
    "        # ✅ checkpoint 기준: val loss (기존 유지)\n",
    "        if va_loss < best_val_loss:\n",
    "            best_val_loss = va_loss\n",
    "            best_val_nmse = va_nmse  # loss 기준으로 best일 때의 nmse 저장\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"best_val_nmse_db\": best_val_nmse,\n",
    "                    \"F_in\": F_in,\n",
    "                    \"F_out\": F_out,\n",
    "                },\n",
    "                ckpt_path\n",
    "            )\n",
    "            print(f\"  ↳ saved {ckpt_path} (best val loss)\")\n",
    "\n",
    "        # (옵션) 만약 \"NMSE(dB) 기준 best\"도 따로 저장하고 싶으면 아래처럼 추가 가능\n",
    "        # if va_nmse < best_val_nmse:\n",
    "        #     best_val_nmse = va_nmse\n",
    "        #     torch.save(..., \"best_nmse_convlstm.pt\")\n",
    "\n",
    "    total_dt = time.time() - total_t0\n",
    "    print(f\"\\nTOTAL TRAIN TIME: {total_dt/60:.2f} min ({total_dt:.1f} sec)\")\n",
    "    print(f\"BEST (by val loss) -> val loss={best_val_loss:.6f}, val nmse(dB)={best_val_nmse:.4f}\")\n",
    "\n",
    "    # ✅ best loss, best nmse, total time 모두 리턴\n",
    "    return best_val_loss, best_val_nmse, total_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ab5d4",
   "metadata": {},
   "source": [
    "#  Optiimizer  / Scheduler 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d404b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # requires_grad=True인 파라미터만 학습\n",
    "# trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "# print(\"trainable params:\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "# optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# # (선택) cosine scheduler\n",
    "# epochs = 1000\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897236b",
   "metadata": {},
   "source": [
    "# 학습 루프 + checkpoint 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "758db97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training: ConvLSTM Early Fusion ===\n",
      "trainable params: 3476032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_676185/2947542360.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_676185/2947542360.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/500] train loss=0.289630, nmse(dB)=0.0518 | val loss=0.261238, nmse(dB)=0.0482 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[02/500] train loss=0.289253, nmse(dB)=0.0446 | val loss=0.260723, nmse(dB)=0.0396 | 85.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[03/500] train loss=0.288630, nmse(dB)=0.0364 | val loss=0.260090, nmse(dB)=0.0290 | 81.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[04/500] train loss=0.287991, nmse(dB)=0.0260 | val loss=0.259203, nmse(dB)=0.0142 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[05/500] train loss=0.287020, nmse(dB)=0.0108 | val loss=0.257797, nmse(dB)=-0.0095 | 81.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[06/500] train loss=0.285249, nmse(dB)=-0.0141 | val loss=0.255410, nmse(dB)=-0.0499 | 82.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[07/500] train loss=0.282736, nmse(dB)=-0.0565 | val loss=0.251406, nmse(dB)=-0.1186 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[08/500] train loss=0.278306, nmse(dB)=-0.1254 | val loss=0.245946, nmse(dB)=-0.2140 | 84.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[09/500] train loss=0.273038, nmse(dB)=-0.2071 | val loss=0.243077, nmse(dB)=-0.2650 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[10/500] train loss=0.270656, nmse(dB)=-0.2411 | val loss=0.240357, nmse(dB)=-0.3139 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[11/500] train loss=0.267240, nmse(dB)=-0.2964 | val loss=0.235786, nmse(dB)=-0.3974 | 82.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[12/500] train loss=0.263015, nmse(dB)=-0.3685 | val loss=0.232239, nmse(dB)=-0.4633 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[13/500] train loss=0.259596, nmse(dB)=-0.4252 | val loss=0.229093, nmse(dB)=-0.5226 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[14/500] train loss=0.256415, nmse(dB)=-0.4807 | val loss=0.225361, nmse(dB)=-0.5940 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[15/500] train loss=0.252604, nmse(dB)=-0.5446 | val loss=0.221261, nmse(dB)=-0.6738 | 83.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[16/500] train loss=0.248683, nmse(dB)=-0.6154 | val loss=0.217213, nmse(dB)=-0.7541 | 86.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[17/500] train loss=0.244493, nmse(dB)=-0.6866 | val loss=0.213031, nmse(dB)=-0.8386 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[18/500] train loss=0.240266, nmse(dB)=-0.7617 | val loss=0.208584, nmse(dB)=-0.9303 | 86.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[19/500] train loss=0.236020, nmse(dB)=-0.8417 | val loss=0.204048, nmse(dB)=-1.0259 | 84.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[20/500] train loss=0.231313, nmse(dB)=-0.9272 | val loss=0.199491, nmse(dB)=-1.1241 | 85.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[21/500] train loss=0.226752, nmse(dB)=-1.0140 | val loss=0.194832, nmse(dB)=-1.2269 | 83.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[22/500] train loss=0.222173, nmse(dB)=-1.1026 | val loss=0.190067, nmse(dB)=-1.3346 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[23/500] train loss=0.217563, nmse(dB)=-1.1957 | val loss=0.185286, nmse(dB)=-1.4454 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[24/500] train loss=0.212730, nmse(dB)=-1.2921 | val loss=0.180494, nmse(dB)=-1.5594 | 84.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[25/500] train loss=0.207961, nmse(dB)=-1.3904 | val loss=0.175681, nmse(dB)=-1.6769 | 88.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[26/500] train loss=0.203222, nmse(dB)=-1.4925 | val loss=0.170866, nmse(dB)=-1.7978 | 85.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[27/500] train loss=0.198557, nmse(dB)=-1.5929 | val loss=0.166052, nmse(dB)=-1.9222 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[28/500] train loss=0.193769, nmse(dB)=-1.7015 | val loss=0.161277, nmse(dB)=-2.0492 | 83.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[29/500] train loss=0.189102, nmse(dB)=-1.8079 | val loss=0.156514, nmse(dB)=-2.1796 | 85.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[30/500] train loss=0.184511, nmse(dB)=-1.9178 | val loss=0.151805, nmse(dB)=-2.3126 | 83.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[31/500] train loss=0.179819, nmse(dB)=-2.0298 | val loss=0.147142, nmse(dB)=-2.4484 | 83.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[32/500] train loss=0.175299, nmse(dB)=-2.1431 | val loss=0.142536, nmse(dB)=-2.5869 | 86.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[33/500] train loss=0.170646, nmse(dB)=-2.2577 | val loss=0.137976, nmse(dB)=-2.7285 | 82.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[34/500] train loss=0.166174, nmse(dB)=-2.3763 | val loss=0.133498, nmse(dB)=-2.8722 | 86.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[35/500] train loss=0.161897, nmse(dB)=-2.4912 | val loss=0.129095, nmse(dB)=-3.0183 | 84.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[36/500] train loss=0.157459, nmse(dB)=-2.6161 | val loss=0.124782, nmse(dB)=-3.1663 | 86.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[37/500] train loss=0.153187, nmse(dB)=-2.7359 | val loss=0.120508, nmse(dB)=-3.3182 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[38/500] train loss=0.149088, nmse(dB)=-2.8551 | val loss=0.116348, nmse(dB)=-3.4714 | 88.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[39/500] train loss=0.145058, nmse(dB)=-2.9761 | val loss=0.112278, nmse(dB)=-3.6266 | 85.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[40/500] train loss=0.141097, nmse(dB)=-3.1016 | val loss=0.108329, nmse(dB)=-3.7828 | 86.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[41/500] train loss=0.137002, nmse(dB)=-3.2324 | val loss=0.104450, nmse(dB)=-3.9419 | 87.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[42/500] train loss=0.133329, nmse(dB)=-3.3531 | val loss=0.100664, nmse(dB)=-4.1031 | 87.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[43/500] train loss=0.129615, nmse(dB)=-3.4816 | val loss=0.096996, nmse(dB)=-4.2651 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[44/500] train loss=0.125973, nmse(dB)=-3.6075 | val loss=0.093424, nmse(dB)=-4.4290 | 82.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[45/500] train loss=0.122467, nmse(dB)=-3.7331 | val loss=0.089953, nmse(dB)=-4.5945 | 86.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[46/500] train loss=0.119230, nmse(dB)=-3.8549 | val loss=0.086603, nmse(dB)=-4.7604 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[47/500] train loss=0.115879, nmse(dB)=-3.9845 | val loss=0.083354, nmse(dB)=-4.9277 | 85.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[48/500] train loss=0.112640, nmse(dB)=-4.1141 | val loss=0.080199, nmse(dB)=-5.0966 | 83.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[49/500] train loss=0.109529, nmse(dB)=-4.2377 | val loss=0.077142, nmse(dB)=-5.2668 | 86.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[50/500] train loss=0.106546, nmse(dB)=-4.3636 | val loss=0.074188, nmse(dB)=-5.4379 | 84.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[51/500] train loss=0.103797, nmse(dB)=-4.4858 | val loss=0.071382, nmse(dB)=-5.6070 | 84.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[52/500] train loss=0.100893, nmse(dB)=-4.6149 | val loss=0.068635, nmse(dB)=-5.7793 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[53/500] train loss=0.098279, nmse(dB)=-4.7346 | val loss=0.065972, nmse(dB)=-5.9531 | 84.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[54/500] train loss=0.095684, nmse(dB)=-4.8584 | val loss=0.063435, nmse(dB)=-6.1255 | 86.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[55/500] train loss=0.093090, nmse(dB)=-4.9858 | val loss=0.060997, nmse(dB)=-6.2980 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[56/500] train loss=0.090889, nmse(dB)=-5.0982 | val loss=0.058657, nmse(dB)=-6.4704 | 86.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[57/500] train loss=0.088428, nmse(dB)=-5.2242 | val loss=0.056384, nmse(dB)=-6.6448 | 84.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[58/500] train loss=0.086255, nmse(dB)=-5.3450 | val loss=0.054238, nmse(dB)=-6.8162 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[59/500] train loss=0.084063, nmse(dB)=-5.4628 | val loss=0.052150, nmse(dB)=-6.9897 | 83.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[60/500] train loss=0.082103, nmse(dB)=-5.5744 | val loss=0.050160, nmse(dB)=-7.1621 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[61/500] train loss=0.080312, nmse(dB)=-5.6858 | val loss=0.048275, nmse(dB)=-7.3319 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[62/500] train loss=0.078340, nmse(dB)=-5.7997 | val loss=0.046447, nmse(dB)=-7.5034 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[63/500] train loss=0.076679, nmse(dB)=-5.9033 | val loss=0.044706, nmse(dB)=-7.6736 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[64/500] train loss=0.074958, nmse(dB)=-6.0117 | val loss=0.043040, nmse(dB)=-7.8429 | 88.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[65/500] train loss=0.073230, nmse(dB)=-6.1247 | val loss=0.041454, nmse(dB)=-8.0106 | 85.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[66/500] train loss=0.071610, nmse(dB)=-6.2349 | val loss=0.039946, nmse(dB)=-8.1765 | 87.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[67/500] train loss=0.070265, nmse(dB)=-6.3276 | val loss=0.038498, nmse(dB)=-8.3423 | 85.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[68/500] train loss=0.068851, nmse(dB)=-6.4292 | val loss=0.037118, nmse(dB)=-8.5065 | 86.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[69/500] train loss=0.067508, nmse(dB)=-6.5269 | val loss=0.035796, nmse(dB)=-8.6702 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[70/500] train loss=0.066172, nmse(dB)=-6.6252 | val loss=0.034544, nmse(dB)=-8.8312 | 87.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[71/500] train loss=0.064922, nmse(dB)=-6.7213 | val loss=0.033359, nmse(dB)=-8.9896 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[72/500] train loss=0.063826, nmse(dB)=-6.8091 | val loss=0.032230, nmse(dB)=-9.1462 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[73/500] train loss=0.062722, nmse(dB)=-6.8957 | val loss=0.031144, nmse(dB)=-9.3028 | 84.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[74/500] train loss=0.061675, nmse(dB)=-6.9849 | val loss=0.030121, nmse(dB)=-9.4557 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[75/500] train loss=0.060627, nmse(dB)=-7.0729 | val loss=0.029148, nmse(dB)=-9.6067 | 84.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[76/500] train loss=0.059696, nmse(dB)=-7.1526 | val loss=0.028219, nmse(dB)=-9.7563 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[77/500] train loss=0.058803, nmse(dB)=-7.2304 | val loss=0.027337, nmse(dB)=-9.9034 | 87.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[78/500] train loss=0.057832, nmse(dB)=-7.3161 | val loss=0.026510, nmse(dB)=-10.0467 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[79/500] train loss=0.057188, nmse(dB)=-7.3803 | val loss=0.025736, nmse(dB)=-10.1853 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[80/500] train loss=0.056402, nmse(dB)=-7.4492 | val loss=0.024966, nmse(dB)=-10.3278 | 89.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[81/500] train loss=0.055721, nmse(dB)=-7.5173 | val loss=0.024247, nmse(dB)=-10.4658 | 87.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[82/500] train loss=0.054886, nmse(dB)=-7.5967 | val loss=0.023575, nmse(dB)=-10.5991 | 87.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[83/500] train loss=0.054286, nmse(dB)=-7.6582 | val loss=0.022946, nmse(dB)=-10.7283 | 86.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[84/500] train loss=0.053759, nmse(dB)=-7.7138 | val loss=0.022343, nmse(dB)=-10.8558 | 88.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[85/500] train loss=0.053050, nmse(dB)=-7.7805 | val loss=0.021752, nmse(dB)=-10.9851 | 85.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[86/500] train loss=0.052512, nmse(dB)=-7.8389 | val loss=0.021203, nmse(dB)=-11.1092 | 88.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[87/500] train loss=0.052077, nmse(dB)=-7.8881 | val loss=0.020694, nmse(dB)=-11.2281 | 86.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[88/500] train loss=0.051460, nmse(dB)=-7.9531 | val loss=0.020219, nmse(dB)=-11.3420 | 85.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[89/500] train loss=0.051084, nmse(dB)=-7.9984 | val loss=0.019757, nmse(dB)=-11.4562 | 88.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[90/500] train loss=0.050544, nmse(dB)=-8.0533 | val loss=0.019296, nmse(dB)=-11.5736 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[91/500] train loss=0.050082, nmse(dB)=-8.1068 | val loss=0.018881, nmse(dB)=-11.6826 | 88.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[92/500] train loss=0.049733, nmse(dB)=-8.1506 | val loss=0.018490, nmse(dB)=-11.7882 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[93/500] train loss=0.049321, nmse(dB)=-8.1952 | val loss=0.018124, nmse(dB)=-11.8900 | 88.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[94/500] train loss=0.049016, nmse(dB)=-8.2309 | val loss=0.017767, nmse(dB)=-11.9918 | 86.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[95/500] train loss=0.048713, nmse(dB)=-8.2713 | val loss=0.017432, nmse(dB)=-12.0898 | 88.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[96/500] train loss=0.048368, nmse(dB)=-8.3115 | val loss=0.017122, nmse(dB)=-12.1831 | 83.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[97/500] train loss=0.048011, nmse(dB)=-8.3571 | val loss=0.016825, nmse(dB)=-12.2748 | 88.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[98/500] train loss=0.047807, nmse(dB)=-8.3824 | val loss=0.016529, nmse(dB)=-12.3683 | 85.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[99/500] train loss=0.047426, nmse(dB)=-8.4229 | val loss=0.016259, nmse(dB)=-12.4560 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[100/500] train loss=0.047198, nmse(dB)=-8.4583 | val loss=0.016013, nmse(dB)=-12.5377 | 84.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[101/500] train loss=0.046900, nmse(dB)=-8.4923 | val loss=0.015782, nmse(dB)=-12.6165 | 88.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[102/500] train loss=0.046616, nmse(dB)=-8.5233 | val loss=0.015540, nmse(dB)=-12.7003 | 85.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[103/500] train loss=0.046515, nmse(dB)=-8.5436 | val loss=0.015324, nmse(dB)=-12.7770 | 88.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[104/500] train loss=0.046318, nmse(dB)=-8.5738 | val loss=0.015117, nmse(dB)=-12.8523 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[105/500] train loss=0.046115, nmse(dB)=-8.5960 | val loss=0.014924, nmse(dB)=-12.9241 | 87.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[106/500] train loss=0.045806, nmse(dB)=-8.6347 | val loss=0.014744, nmse(dB)=-12.9925 | 84.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[107/500] train loss=0.045659, nmse(dB)=-8.6522 | val loss=0.014574, nmse(dB)=-13.0582 | 86.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[108/500] train loss=0.045558, nmse(dB)=-8.6713 | val loss=0.014405, nmse(dB)=-13.1250 | 89.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[109/500] train loss=0.045330, nmse(dB)=-8.6998 | val loss=0.014249, nmse(dB)=-13.1875 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[110/500] train loss=0.045207, nmse(dB)=-8.7180 | val loss=0.014099, nmse(dB)=-13.2488 | 87.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[111/500] train loss=0.045007, nmse(dB)=-8.7412 | val loss=0.013957, nmse(dB)=-13.3075 | 86.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[112/500] train loss=0.044849, nmse(dB)=-8.7610 | val loss=0.013821, nmse(dB)=-13.3654 | 87.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[113/500] train loss=0.044807, nmse(dB)=-8.7754 | val loss=0.013697, nmse(dB)=-13.4188 | 85.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[114/500] train loss=0.044655, nmse(dB)=-8.7935 | val loss=0.013577, nmse(dB)=-13.4716 | 90.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[115/500] train loss=0.044552, nmse(dB)=-8.8086 | val loss=0.013459, nmse(dB)=-13.5245 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[116/500] train loss=0.044423, nmse(dB)=-8.8251 | val loss=0.013353, nmse(dB)=-13.5728 | 87.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[117/500] train loss=0.044287, nmse(dB)=-8.8414 | val loss=0.013246, nmse(dB)=-13.6222 | 91.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[118/500] train loss=0.044302, nmse(dB)=-8.8465 | val loss=0.013147, nmse(dB)=-13.6680 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[119/500] train loss=0.044156, nmse(dB)=-8.8618 | val loss=0.013046, nmse(dB)=-13.7157 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[120/500] train loss=0.044041, nmse(dB)=-8.8797 | val loss=0.012965, nmse(dB)=-13.7547 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[121/500] train loss=0.043999, nmse(dB)=-8.8911 | val loss=0.012897, nmse(dB)=-13.7882 | 87.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[122/500] train loss=0.043922, nmse(dB)=-8.8986 | val loss=0.012802, nmse(dB)=-13.8349 | 87.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[123/500] train loss=0.043761, nmse(dB)=-8.9215 | val loss=0.012728, nmse(dB)=-13.8717 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[124/500] train loss=0.043800, nmse(dB)=-8.9208 | val loss=0.012649, nmse(dB)=-13.9116 | 89.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[125/500] train loss=0.043586, nmse(dB)=-8.9460 | val loss=0.012579, nmse(dB)=-13.9474 | 85.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[126/500] train loss=0.043618, nmse(dB)=-8.9486 | val loss=0.012518, nmse(dB)=-13.9799 | 88.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[127/500] train loss=0.043483, nmse(dB)=-8.9628 | val loss=0.012464, nmse(dB)=-14.0077 | 85.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[128/500] train loss=0.043438, nmse(dB)=-8.9714 | val loss=0.012401, nmse(dB)=-14.0416 | 85.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[129/500] train loss=0.043388, nmse(dB)=-8.9813 | val loss=0.012349, nmse(dB)=-14.0702 | 88.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[130/500] train loss=0.043358, nmse(dB)=-8.9829 | val loss=0.012286, nmse(dB)=-14.1046 | 85.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[131/500] train loss=0.043304, nmse(dB)=-8.9902 | val loss=0.012233, nmse(dB)=-14.1333 | 88.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[132/500] train loss=0.043340, nmse(dB)=-8.9889 | val loss=0.012187, nmse(dB)=-14.1576 | 84.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[133/500] train loss=0.043193, nmse(dB)=-9.0049 | val loss=0.012144, nmse(dB)=-14.1812 | 89.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[134/500] train loss=0.043169, nmse(dB)=-9.0086 | val loss=0.012101, nmse(dB)=-14.2059 | 86.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[135/500] train loss=0.043033, nmse(dB)=-9.0215 | val loss=0.012053, nmse(dB)=-14.2344 | 88.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[136/500] train loss=0.043063, nmse(dB)=-9.0255 | val loss=0.012017, nmse(dB)=-14.2549 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[137/500] train loss=0.042913, nmse(dB)=-9.0416 | val loss=0.011979, nmse(dB)=-14.2765 | 88.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[138/500] train loss=0.043004, nmse(dB)=-9.0363 | val loss=0.011941, nmse(dB)=-14.2987 | 85.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[139/500] train loss=0.042898, nmse(dB)=-9.0492 | val loss=0.011907, nmse(dB)=-14.3185 | 88.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[140/500] train loss=0.042889, nmse(dB)=-9.0481 | val loss=0.011871, nmse(dB)=-14.3396 | 84.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[141/500] train loss=0.042840, nmse(dB)=-9.0538 | val loss=0.011839, nmse(dB)=-14.3583 | 85.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[142/500] train loss=0.042831, nmse(dB)=-9.0570 | val loss=0.011808, nmse(dB)=-14.3762 | 88.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[143/500] train loss=0.042833, nmse(dB)=-9.0566 | val loss=0.011777, nmse(dB)=-14.3953 | 85.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[144/500] train loss=0.042785, nmse(dB)=-9.0652 | val loss=0.011753, nmse(dB)=-14.4104 | 88.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[145/500] train loss=0.042754, nmse(dB)=-9.0689 | val loss=0.011733, nmse(dB)=-14.4226 | 83.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[146/500] train loss=0.042776, nmse(dB)=-9.0700 | val loss=0.011712, nmse(dB)=-14.4362 | 88.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[147/500] train loss=0.042734, nmse(dB)=-9.0724 | val loss=0.011684, nmse(dB)=-14.4535 | 84.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[148/500] train loss=0.042671, nmse(dB)=-9.0831 | val loss=0.011667, nmse(dB)=-14.4635 | 84.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[149/500] train loss=0.042790, nmse(dB)=-9.0710 | val loss=0.011646, nmse(dB)=-14.4767 | 87.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[150/500] train loss=0.042671, nmse(dB)=-9.0855 | val loss=0.011618, nmse(dB)=-14.4942 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[151/500] train loss=0.042630, nmse(dB)=-9.0897 | val loss=0.011599, nmse(dB)=-14.5064 | 85.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[152/500] train loss=0.042556, nmse(dB)=-9.0996 | val loss=0.011583, nmse(dB)=-14.5161 | 82.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[153/500] train loss=0.042636, nmse(dB)=-9.0945 | val loss=0.011566, nmse(dB)=-14.5272 | 82.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[154/500] train loss=0.042462, nmse(dB)=-9.1091 | val loss=0.011547, nmse(dB)=-14.5401 | 87.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[155/500] train loss=0.042566, nmse(dB)=-9.1039 | val loss=0.011534, nmse(dB)=-14.5484 | 83.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[156/500] train loss=0.042625, nmse(dB)=-9.0959 | val loss=0.011516, nmse(dB)=-14.5598 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[157/500] train loss=0.042544, nmse(dB)=-9.1034 | val loss=0.011495, nmse(dB)=-14.5729 | 83.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[158/500] train loss=0.042543, nmse(dB)=-9.1085 | val loss=0.011487, nmse(dB)=-14.5786 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[159/500] train loss=0.042524, nmse(dB)=-9.1059 | val loss=0.011469, nmse(dB)=-14.5899 | 86.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[160/500] train loss=0.042532, nmse(dB)=-9.1133 | val loss=0.011465, nmse(dB)=-14.5937 | 83.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[161/500] train loss=0.042435, nmse(dB)=-9.1177 | val loss=0.011451, nmse(dB)=-14.6034 | 86.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[162/500] train loss=0.042416, nmse(dB)=-9.1199 | val loss=0.011435, nmse(dB)=-14.6138 | 83.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[163/500] train loss=0.042488, nmse(dB)=-9.1136 | val loss=0.011418, nmse(dB)=-14.6240 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[164/500] train loss=0.042382, nmse(dB)=-9.1287 | val loss=0.011411, nmse(dB)=-14.6290 | 83.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[165/500] train loss=0.042467, nmse(dB)=-9.1214 | val loss=0.011411, nmse(dB)=-14.6287 | 84.0s\n",
      "[166/500] train loss=0.042495, nmse(dB)=-9.1158 | val loss=0.011392, nmse(dB)=-14.6416 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[167/500] train loss=0.042372, nmse(dB)=-9.1275 | val loss=0.011378, nmse(dB)=-14.6506 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[168/500] train loss=0.042466, nmse(dB)=-9.1175 | val loss=0.011370, nmse(dB)=-14.6553 | 87.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[169/500] train loss=0.042413, nmse(dB)=-9.1206 | val loss=0.011359, nmse(dB)=-14.6618 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[170/500] train loss=0.042451, nmse(dB)=-9.1215 | val loss=0.011349, nmse(dB)=-14.6693 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[171/500] train loss=0.042356, nmse(dB)=-9.1315 | val loss=0.011342, nmse(dB)=-14.6744 | 86.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[172/500] train loss=0.042366, nmse(dB)=-9.1336 | val loss=0.011347, nmse(dB)=-14.6717 | 83.8s\n",
      "[173/500] train loss=0.042390, nmse(dB)=-9.1284 | val loss=0.011334, nmse(dB)=-14.6810 | 87.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[174/500] train loss=0.042389, nmse(dB)=-9.1315 | val loss=0.011320, nmse(dB)=-14.6903 | 84.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[175/500] train loss=0.042310, nmse(dB)=-9.1363 | val loss=0.011310, nmse(dB)=-14.6973 | 83.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[176/500] train loss=0.042353, nmse(dB)=-9.1381 | val loss=0.011308, nmse(dB)=-14.6993 | 87.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[177/500] train loss=0.042297, nmse(dB)=-9.1405 | val loss=0.011305, nmse(dB)=-14.7007 | 83.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[178/500] train loss=0.042369, nmse(dB)=-9.1304 | val loss=0.011291, nmse(dB)=-14.7107 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[179/500] train loss=0.042277, nmse(dB)=-9.1405 | val loss=0.011283, nmse(dB)=-14.7168 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[180/500] train loss=0.042283, nmse(dB)=-9.1418 | val loss=0.011282, nmse(dB)=-14.7169 | 85.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[181/500] train loss=0.042350, nmse(dB)=-9.1395 | val loss=0.011283, nmse(dB)=-14.7166 | 86.6s\n",
      "[182/500] train loss=0.042321, nmse(dB)=-9.1400 | val loss=0.011283, nmse(dB)=-14.7179 | 83.9s\n",
      "[183/500] train loss=0.042387, nmse(dB)=-9.1366 | val loss=0.011273, nmse(dB)=-14.7249 | 87.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[184/500] train loss=0.042247, nmse(dB)=-9.1455 | val loss=0.011263, nmse(dB)=-14.7317 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[185/500] train loss=0.042229, nmse(dB)=-9.1483 | val loss=0.011258, nmse(dB)=-14.7344 | 82.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[186/500] train loss=0.042174, nmse(dB)=-9.1533 | val loss=0.011255, nmse(dB)=-14.7360 | 86.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[187/500] train loss=0.042207, nmse(dB)=-9.1487 | val loss=0.011249, nmse(dB)=-14.7401 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[188/500] train loss=0.042242, nmse(dB)=-9.1442 | val loss=0.011246, nmse(dB)=-14.7410 | 83.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[189/500] train loss=0.042134, nmse(dB)=-9.1563 | val loss=0.011241, nmse(dB)=-14.7457 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[190/500] train loss=0.042274, nmse(dB)=-9.1425 | val loss=0.011237, nmse(dB)=-14.7488 | 83.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[191/500] train loss=0.042261, nmse(dB)=-9.1482 | val loss=0.011237, nmse(dB)=-14.7490 | 83.2s\n",
      "[192/500] train loss=0.042324, nmse(dB)=-9.1399 | val loss=0.011236, nmse(dB)=-14.7499 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[193/500] train loss=0.042259, nmse(dB)=-9.1460 | val loss=0.011228, nmse(dB)=-14.7547 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[194/500] train loss=0.042203, nmse(dB)=-9.1535 | val loss=0.011224, nmse(dB)=-14.7579 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[195/500] train loss=0.042212, nmse(dB)=-9.1510 | val loss=0.011219, nmse(dB)=-14.7615 | 83.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[196/500] train loss=0.042158, nmse(dB)=-9.1547 | val loss=0.011215, nmse(dB)=-14.7642 | 83.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[197/500] train loss=0.042262, nmse(dB)=-9.1474 | val loss=0.011214, nmse(dB)=-14.7642 | 86.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[198/500] train loss=0.042272, nmse(dB)=-9.1502 | val loss=0.011210, nmse(dB)=-14.7670 | 83.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[199/500] train loss=0.042204, nmse(dB)=-9.1503 | val loss=0.011203, nmse(dB)=-14.7722 | 83.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[200/500] train loss=0.042325, nmse(dB)=-9.1466 | val loss=0.011199, nmse(dB)=-14.7750 | 87.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[201/500] train loss=0.042321, nmse(dB)=-9.1429 | val loss=0.011196, nmse(dB)=-14.7777 | 83.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[202/500] train loss=0.042204, nmse(dB)=-9.1540 | val loss=0.011195, nmse(dB)=-14.7785 | 83.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[203/500] train loss=0.042262, nmse(dB)=-9.1501 | val loss=0.011209, nmse(dB)=-14.7682 | 87.2s\n",
      "[204/500] train loss=0.042291, nmse(dB)=-9.1464 | val loss=0.011194, nmse(dB)=-14.7788 | 83.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[205/500] train loss=0.042184, nmse(dB)=-9.1526 | val loss=0.011191, nmse(dB)=-14.7809 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[206/500] train loss=0.042241, nmse(dB)=-9.1518 | val loss=0.011191, nmse(dB)=-14.7815 | 83.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[207/500] train loss=0.042206, nmse(dB)=-9.1567 | val loss=0.011196, nmse(dB)=-14.7792 | 83.4s\n",
      "[208/500] train loss=0.042140, nmse(dB)=-9.1554 | val loss=0.011189, nmse(dB)=-14.7836 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[209/500] train loss=0.042182, nmse(dB)=-9.1545 | val loss=0.011186, nmse(dB)=-14.7860 | 83.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[210/500] train loss=0.042189, nmse(dB)=-9.1525 | val loss=0.011185, nmse(dB)=-14.7861 | 83.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[211/500] train loss=0.042133, nmse(dB)=-9.1553 | val loss=0.011183, nmse(dB)=-14.7884 | 87.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[212/500] train loss=0.042271, nmse(dB)=-9.1471 | val loss=0.011182, nmse(dB)=-14.7891 | 83.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[213/500] train loss=0.042210, nmse(dB)=-9.1524 | val loss=0.011179, nmse(dB)=-14.7916 | 83.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[214/500] train loss=0.042277, nmse(dB)=-9.1452 | val loss=0.011176, nmse(dB)=-14.7935 | 87.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[215/500] train loss=0.042167, nmse(dB)=-9.1556 | val loss=0.011183, nmse(dB)=-14.7884 | 83.5s\n",
      "[216/500] train loss=0.042147, nmse(dB)=-9.1586 | val loss=0.011186, nmse(dB)=-14.7867 | 87.1s\n",
      "[217/500] train loss=0.042179, nmse(dB)=-9.1535 | val loss=0.011178, nmse(dB)=-14.7928 | 83.1s\n",
      "[218/500] train loss=0.042060, nmse(dB)=-9.1658 | val loss=0.011173, nmse(dB)=-14.7961 | 83.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[219/500] train loss=0.042095, nmse(dB)=-9.1657 | val loss=0.011173, nmse(dB)=-14.7954 | 87.0s\n",
      "[220/500] train loss=0.042131, nmse(dB)=-9.1545 | val loss=0.011170, nmse(dB)=-14.7967 | 83.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[221/500] train loss=0.042084, nmse(dB)=-9.1646 | val loss=0.011170, nmse(dB)=-14.7962 | 83.3s\n",
      "[222/500] train loss=0.042040, nmse(dB)=-9.1695 | val loss=0.011175, nmse(dB)=-14.7925 | 87.1s\n",
      "[223/500] train loss=0.042059, nmse(dB)=-9.1648 | val loss=0.011172, nmse(dB)=-14.7950 | 83.1s\n",
      "[224/500] train loss=0.042163, nmse(dB)=-9.1543 | val loss=0.011168, nmse(dB)=-14.7985 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[225/500] train loss=0.042089, nmse(dB)=-9.1597 | val loss=0.011160, nmse(dB)=-14.8047 | 83.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[226/500] train loss=0.042102, nmse(dB)=-9.1624 | val loss=0.011161, nmse(dB)=-14.8042 | 83.5s\n",
      "[227/500] train loss=0.042119, nmse(dB)=-9.1567 | val loss=0.011158, nmse(dB)=-14.8057 | 87.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[228/500] train loss=0.041956, nmse(dB)=-9.1710 | val loss=0.011154, nmse(dB)=-14.8078 | 83.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[229/500] train loss=0.042102, nmse(dB)=-9.1586 | val loss=0.011158, nmse(dB)=-14.8050 | 83.4s\n",
      "[230/500] train loss=0.042101, nmse(dB)=-9.1601 | val loss=0.011159, nmse(dB)=-14.8054 | 86.2s\n",
      "[231/500] train loss=0.042023, nmse(dB)=-9.1670 | val loss=0.011159, nmse(dB)=-14.8059 | 87.5s\n",
      "[232/500] train loss=0.041969, nmse(dB)=-9.1733 | val loss=0.011158, nmse(dB)=-14.8069 | 85.8s\n",
      "[233/500] train loss=0.041994, nmse(dB)=-9.1687 | val loss=0.011152, nmse(dB)=-14.8103 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[234/500] train loss=0.042003, nmse(dB)=-9.1676 | val loss=0.011151, nmse(dB)=-14.8113 | 85.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[235/500] train loss=0.041937, nmse(dB)=-9.1743 | val loss=0.011149, nmse(dB)=-14.8124 | 84.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[236/500] train loss=0.042021, nmse(dB)=-9.1622 | val loss=0.011145, nmse(dB)=-14.8147 | 89.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[237/500] train loss=0.041971, nmse(dB)=-9.1676 | val loss=0.011146, nmse(dB)=-14.8137 | 86.1s\n",
      "[238/500] train loss=0.041906, nmse(dB)=-9.1740 | val loss=0.011146, nmse(dB)=-14.8146 | 89.0s\n",
      "[239/500] train loss=0.041934, nmse(dB)=-9.1705 | val loss=0.011147, nmse(dB)=-14.8142 | 85.4s\n",
      "[240/500] train loss=0.041898, nmse(dB)=-9.1726 | val loss=0.011143, nmse(dB)=-14.8169 | 90.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[241/500] train loss=0.041854, nmse(dB)=-9.1800 | val loss=0.011146, nmse(dB)=-14.8146 | 87.9s\n",
      "[242/500] train loss=0.041849, nmse(dB)=-9.1803 | val loss=0.011145, nmse(dB)=-14.8166 | 90.7s\n",
      "[243/500] train loss=0.041853, nmse(dB)=-9.1796 | val loss=0.011146, nmse(dB)=-14.8155 | 86.9s\n",
      "[244/500] train loss=0.041852, nmse(dB)=-9.1764 | val loss=0.011142, nmse(dB)=-14.8177 | 92.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[245/500] train loss=0.041864, nmse(dB)=-9.1794 | val loss=0.011143, nmse(dB)=-14.8176 | 87.1s\n",
      "[246/500] train loss=0.041862, nmse(dB)=-9.1781 | val loss=0.011145, nmse(dB)=-14.8162 | 85.7s\n",
      "[247/500] train loss=0.041881, nmse(dB)=-9.1725 | val loss=0.011143, nmse(dB)=-14.8177 | 90.1s\n",
      "[248/500] train loss=0.041743, nmse(dB)=-9.1843 | val loss=0.011141, nmse(dB)=-14.8182 | 85.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[249/500] train loss=0.041790, nmse(dB)=-9.1811 | val loss=0.011138, nmse(dB)=-14.8207 | 90.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[250/500] train loss=0.041833, nmse(dB)=-9.1739 | val loss=0.011137, nmse(dB)=-14.8209 | 86.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[251/500] train loss=0.041783, nmse(dB)=-9.1811 | val loss=0.011139, nmse(dB)=-14.8197 | 89.9s\n",
      "[252/500] train loss=0.041868, nmse(dB)=-9.1754 | val loss=0.011141, nmse(dB)=-14.8187 | 85.8s\n",
      "[253/500] train loss=0.041701, nmse(dB)=-9.1875 | val loss=0.011137, nmse(dB)=-14.8213 | 91.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[254/500] train loss=0.041753, nmse(dB)=-9.1845 | val loss=0.011134, nmse(dB)=-14.8243 | 87.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[255/500] train loss=0.041736, nmse(dB)=-9.1824 | val loss=0.011134, nmse(dB)=-14.8249 | 91.0s\n",
      "[256/500] train loss=0.041690, nmse(dB)=-9.1869 | val loss=0.011133, nmse(dB)=-14.8252 | 86.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[257/500] train loss=0.041735, nmse(dB)=-9.1833 | val loss=0.011135, nmse(dB)=-14.8230 | 90.2s\n",
      "[258/500] train loss=0.041798, nmse(dB)=-9.1775 | val loss=0.011134, nmse(dB)=-14.8230 | 86.9s\n",
      "[259/500] train loss=0.041726, nmse(dB)=-9.1822 | val loss=0.011132, nmse(dB)=-14.8248 | 84.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[260/500] train loss=0.041761, nmse(dB)=-9.1808 | val loss=0.011133, nmse(dB)=-14.8245 | 87.2s\n",
      "[261/500] train loss=0.041685, nmse(dB)=-9.1869 | val loss=0.011136, nmse(dB)=-14.8239 | 83.8s\n",
      "[262/500] train loss=0.041650, nmse(dB)=-9.1856 | val loss=0.011133, nmse(dB)=-14.8251 | 88.0s\n",
      "[263/500] train loss=0.041624, nmse(dB)=-9.1901 | val loss=0.011133, nmse(dB)=-14.8240 | 84.2s\n",
      "[264/500] train loss=0.041692, nmse(dB)=-9.1864 | val loss=0.011133, nmse(dB)=-14.8238 | 84.0s\n",
      "[265/500] train loss=0.041636, nmse(dB)=-9.1924 | val loss=0.011132, nmse(dB)=-14.8255 | 88.1s\n",
      "[266/500] train loss=0.041612, nmse(dB)=-9.1883 | val loss=0.011136, nmse(dB)=-14.8234 | 83.7s\n",
      "[267/500] train loss=0.041607, nmse(dB)=-9.1935 | val loss=0.011132, nmse(dB)=-14.8261 | 88.0s\n",
      "[268/500] train loss=0.041612, nmse(dB)=-9.1944 | val loss=0.011133, nmse(dB)=-14.8251 | 84.5s\n",
      "[269/500] train loss=0.041540, nmse(dB)=-9.1983 | val loss=0.011132, nmse(dB)=-14.8257 | 85.1s\n",
      "[270/500] train loss=0.041539, nmse(dB)=-9.1935 | val loss=0.011132, nmse(dB)=-14.8260 | 87.9s\n",
      "[271/500] train loss=0.041612, nmse(dB)=-9.1879 | val loss=0.011133, nmse(dB)=-14.8256 | 83.8s\n",
      "[272/500] train loss=0.041616, nmse(dB)=-9.1921 | val loss=0.011131, nmse(dB)=-14.8268 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[273/500] train loss=0.041556, nmse(dB)=-9.1948 | val loss=0.011132, nmse(dB)=-14.8256 | 87.9s\n",
      "[274/500] train loss=0.041556, nmse(dB)=-9.1922 | val loss=0.011131, nmse(dB)=-14.8261 | 84.9s\n",
      "[275/500] train loss=0.041508, nmse(dB)=-9.2014 | val loss=0.011132, nmse(dB)=-14.8262 | 88.7s\n",
      "[276/500] train loss=0.041540, nmse(dB)=-9.1908 | val loss=0.011130, nmse(dB)=-14.8271 | 83.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[277/500] train loss=0.041582, nmse(dB)=-9.1889 | val loss=0.011129, nmse(dB)=-14.8269 | 83.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[278/500] train loss=0.041510, nmse(dB)=-9.1976 | val loss=0.011129, nmse(dB)=-14.8270 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[279/500] train loss=0.041548, nmse(dB)=-9.1946 | val loss=0.011132, nmse(dB)=-14.8259 | 86.1s\n",
      "[280/500] train loss=0.041377, nmse(dB)=-9.2044 | val loss=0.011132, nmse(dB)=-14.8259 | 87.6s\n",
      "[281/500] train loss=0.041428, nmse(dB)=-9.2043 | val loss=0.011130, nmse(dB)=-14.8254 | 83.3s\n",
      "[282/500] train loss=0.041496, nmse(dB)=-9.1989 | val loss=0.011131, nmse(dB)=-14.8254 | 83.5s\n",
      "[283/500] train loss=0.041388, nmse(dB)=-9.2055 | val loss=0.011139, nmse(dB)=-14.8212 | 87.9s\n",
      "[284/500] train loss=0.041354, nmse(dB)=-9.2089 | val loss=0.011136, nmse(dB)=-14.8234 | 86.3s\n",
      "[285/500] train loss=0.041432, nmse(dB)=-9.1985 | val loss=0.011131, nmse(dB)=-14.8252 | 89.6s\n",
      "[286/500] train loss=0.041438, nmse(dB)=-9.1989 | val loss=0.011131, nmse(dB)=-14.8248 | 86.5s\n",
      "[287/500] train loss=0.041450, nmse(dB)=-9.2002 | val loss=0.011132, nmse(dB)=-14.8247 | 88.3s\n",
      "[288/500] train loss=0.041452, nmse(dB)=-9.1958 | val loss=0.011135, nmse(dB)=-14.8236 | 85.1s\n",
      "[289/500] train loss=0.041345, nmse(dB)=-9.2065 | val loss=0.011136, nmse(dB)=-14.8225 | 89.9s\n",
      "[290/500] train loss=0.041376, nmse(dB)=-9.2029 | val loss=0.011136, nmse(dB)=-14.8224 | 83.4s\n",
      "[291/500] train loss=0.041420, nmse(dB)=-9.2027 | val loss=0.011133, nmse(dB)=-14.8242 | 85.0s\n",
      "[292/500] train loss=0.041326, nmse(dB)=-9.2103 | val loss=0.011135, nmse(dB)=-14.8238 | 87.6s\n",
      "[293/500] train loss=0.041330, nmse(dB)=-9.2075 | val loss=0.011149, nmse(dB)=-14.8170 | 84.5s\n",
      "[294/500] train loss=0.041314, nmse(dB)=-9.2068 | val loss=0.011144, nmse(dB)=-14.8203 | 88.0s\n",
      "[295/500] train loss=0.041418, nmse(dB)=-9.2022 | val loss=0.011132, nmse(dB)=-14.8251 | 84.5s\n",
      "[296/500] train loss=0.041370, nmse(dB)=-9.2037 | val loss=0.011129, nmse(dB)=-14.8263 | 87.8s\n",
      "[297/500] train loss=0.041302, nmse(dB)=-9.2129 | val loss=0.011139, nmse(dB)=-14.8222 | 85.2s\n",
      "[298/500] train loss=0.041287, nmse(dB)=-9.2124 | val loss=0.011149, nmse(dB)=-14.8172 | 87.6s\n",
      "[299/500] train loss=0.041233, nmse(dB)=-9.2142 | val loss=0.011154, nmse(dB)=-14.8150 | 85.3s\n",
      "[300/500] train loss=0.041296, nmse(dB)=-9.2083 | val loss=0.011142, nmse(dB)=-14.8213 | 86.2s\n",
      "[301/500] train loss=0.041356, nmse(dB)=-9.2035 | val loss=0.011130, nmse(dB)=-14.8264 | 87.5s\n",
      "[302/500] train loss=0.041380, nmse(dB)=-9.2007 | val loss=0.011132, nmse(dB)=-14.8261 | 87.2s\n",
      "[303/500] train loss=0.041233, nmse(dB)=-9.2132 | val loss=0.011140, nmse(dB)=-14.8229 | 89.3s\n",
      "[304/500] train loss=0.041280, nmse(dB)=-9.2074 | val loss=0.011147, nmse(dB)=-14.8190 | 91.5s\n",
      "[305/500] train loss=0.041332, nmse(dB)=-9.2016 | val loss=0.011138, nmse(dB)=-14.8237 | 88.0s\n",
      "[306/500] train loss=0.041288, nmse(dB)=-9.2091 | val loss=0.011133, nmse(dB)=-14.8261 | 90.7s\n",
      "[307/500] train loss=0.041227, nmse(dB)=-9.2157 | val loss=0.011136, nmse(dB)=-14.8248 | 85.4s\n",
      "[308/500] train loss=0.041312, nmse(dB)=-9.2073 | val loss=0.011136, nmse(dB)=-14.8244 | 88.2s\n",
      "[309/500] train loss=0.041260, nmse(dB)=-9.2089 | val loss=0.011138, nmse(dB)=-14.8240 | 86.4s\n",
      "[310/500] train loss=0.041273, nmse(dB)=-9.2071 | val loss=0.011135, nmse(dB)=-14.8251 | 88.5s\n",
      "[311/500] train loss=0.041085, nmse(dB)=-9.2293 | val loss=0.011136, nmse(dB)=-14.8249 | 85.4s\n",
      "[312/500] train loss=0.041183, nmse(dB)=-9.2151 | val loss=0.011141, nmse(dB)=-14.8226 | 85.6s\n",
      "[313/500] train loss=0.041104, nmse(dB)=-9.2185 | val loss=0.011146, nmse(dB)=-14.8201 | 88.8s\n",
      "[314/500] train loss=0.041098, nmse(dB)=-9.2246 | val loss=0.011142, nmse(dB)=-14.8222 | 85.6s\n",
      "[315/500] train loss=0.041168, nmse(dB)=-9.2177 | val loss=0.011133, nmse(dB)=-14.8270 | 89.2s\n",
      "[316/500] train loss=0.041096, nmse(dB)=-9.2259 | val loss=0.011133, nmse(dB)=-14.8264 | 85.8s\n",
      "[317/500] train loss=0.041108, nmse(dB)=-9.2182 | val loss=0.011142, nmse(dB)=-14.8227 | 88.5s\n",
      "[318/500] train loss=0.041083, nmse(dB)=-9.2250 | val loss=0.011143, nmse(dB)=-14.8223 | 85.6s\n",
      "[319/500] train loss=0.041140, nmse(dB)=-9.2180 | val loss=0.011139, nmse(dB)=-14.8237 | 88.7s\n",
      "[320/500] train loss=0.041156, nmse(dB)=-9.2186 | val loss=0.011131, nmse(dB)=-14.8282 | 85.4s\n",
      "[321/500] train loss=0.041165, nmse(dB)=-9.2148 | val loss=0.011131, nmse(dB)=-14.8273 | 88.7s\n",
      "[322/500] train loss=0.041116, nmse(dB)=-9.2212 | val loss=0.011131, nmse(dB)=-14.8276 | 85.3s\n",
      "[323/500] train loss=0.041146, nmse(dB)=-9.2147 | val loss=0.011144, nmse(dB)=-14.8201 | 88.7s\n",
      "[324/500] train loss=0.041085, nmse(dB)=-9.2227 | val loss=0.011149, nmse(dB)=-14.8179 | 85.4s\n",
      "[325/500] train loss=0.041075, nmse(dB)=-9.2256 | val loss=0.011134, nmse(dB)=-14.8261 | 88.5s\n",
      "[326/500] train loss=0.040969, nmse(dB)=-9.2311 | val loss=0.011127, nmse(dB)=-14.8300 | 85.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[327/500] train loss=0.040995, nmse(dB)=-9.2297 | val loss=0.011129, nmse(dB)=-14.8290 | 88.2s\n",
      "[328/500] train loss=0.041084, nmse(dB)=-9.2255 | val loss=0.011133, nmse(dB)=-14.8275 | 85.7s\n",
      "[329/500] train loss=0.041035, nmse(dB)=-9.2264 | val loss=0.011138, nmse(dB)=-14.8249 | 88.7s\n",
      "[330/500] train loss=0.041039, nmse(dB)=-9.2269 | val loss=0.011139, nmse(dB)=-14.8244 | 85.5s\n",
      "[331/500] train loss=0.041004, nmse(dB)=-9.2298 | val loss=0.011138, nmse(dB)=-14.8253 | 88.6s\n",
      "[332/500] train loss=0.040963, nmse(dB)=-9.2273 | val loss=0.011131, nmse(dB)=-14.8288 | 85.6s\n",
      "[333/500] train loss=0.040914, nmse(dB)=-9.2350 | val loss=0.011128, nmse(dB)=-14.8305 | 87.4s\n",
      "[334/500] train loss=0.040941, nmse(dB)=-9.2366 | val loss=0.011128, nmse(dB)=-14.8308 | 84.9s\n",
      "[335/500] train loss=0.040950, nmse(dB)=-9.2298 | val loss=0.011129, nmse(dB)=-14.8299 | 84.9s\n",
      "[336/500] train loss=0.040902, nmse(dB)=-9.2369 | val loss=0.011132, nmse(dB)=-14.8287 | 87.4s\n",
      "[337/500] train loss=0.040860, nmse(dB)=-9.2389 | val loss=0.011133, nmse(dB)=-14.8281 | 85.2s\n",
      "[338/500] train loss=0.040995, nmse(dB)=-9.2314 | val loss=0.011134, nmse(dB)=-14.8276 | 87.0s\n",
      "[339/500] train loss=0.040797, nmse(dB)=-9.2450 | val loss=0.011133, nmse(dB)=-14.8280 | 84.5s\n",
      "[340/500] train loss=0.040867, nmse(dB)=-9.2396 | val loss=0.011129, nmse(dB)=-14.8307 | 87.6s\n",
      "[341/500] train loss=0.040881, nmse(dB)=-9.2385 | val loss=0.011127, nmse(dB)=-14.8319 | 84.7s\n",
      "[342/500] train loss=0.040907, nmse(dB)=-9.2351 | val loss=0.011126, nmse(dB)=-14.8325 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[343/500] train loss=0.040938, nmse(dB)=-9.2285 | val loss=0.011122, nmse(dB)=-14.8345 | 84.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[344/500] train loss=0.040791, nmse(dB)=-9.2464 | val loss=0.011123, nmse(dB)=-14.8344 | 84.7s\n",
      "[345/500] train loss=0.040924, nmse(dB)=-9.2365 | val loss=0.011122, nmse(dB)=-14.8353 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[346/500] train loss=0.040800, nmse(dB)=-9.2417 | val loss=0.011122, nmse(dB)=-14.8353 | 84.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[347/500] train loss=0.040883, nmse(dB)=-9.2361 | val loss=0.011124, nmse(dB)=-14.8342 | 87.4s\n",
      "[348/500] train loss=0.040944, nmse(dB)=-9.2306 | val loss=0.011126, nmse(dB)=-14.8329 | 84.8s\n",
      "[349/500] train loss=0.040756, nmse(dB)=-9.2504 | val loss=0.011132, nmse(dB)=-14.8297 | 87.8s\n",
      "[350/500] train loss=0.040878, nmse(dB)=-9.2348 | val loss=0.011132, nmse(dB)=-14.8295 | 84.7s\n",
      "[351/500] train loss=0.040889, nmse(dB)=-9.2363 | val loss=0.011126, nmse(dB)=-14.8328 | 88.0s\n",
      "[352/500] train loss=0.040824, nmse(dB)=-9.2354 | val loss=0.011122, nmse(dB)=-14.8348 | 84.8s\n",
      "[353/500] train loss=0.040778, nmse(dB)=-9.2442 | val loss=0.011121, nmse(dB)=-14.8348 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[354/500] train loss=0.040761, nmse(dB)=-9.2457 | val loss=0.011121, nmse(dB)=-14.8352 | 84.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[355/500] train loss=0.040894, nmse(dB)=-9.2321 | val loss=0.011124, nmse(dB)=-14.8339 | 85.0s\n",
      "[356/500] train loss=0.040704, nmse(dB)=-9.2462 | val loss=0.011125, nmse(dB)=-14.8335 | 87.3s\n",
      "[357/500] train loss=0.040866, nmse(dB)=-9.2370 | val loss=0.011124, nmse(dB)=-14.8341 | 84.8s\n",
      "[358/500] train loss=0.040778, nmse(dB)=-9.2421 | val loss=0.011122, nmse(dB)=-14.8356 | 87.7s\n",
      "[359/500] train loss=0.040837, nmse(dB)=-9.2365 | val loss=0.011121, nmse(dB)=-14.8361 | 84.6s\n",
      "[360/500] train loss=0.040767, nmse(dB)=-9.2479 | val loss=0.011121, nmse(dB)=-14.8364 | 87.8s\n",
      "[361/500] train loss=0.040832, nmse(dB)=-9.2401 | val loss=0.011120, nmse(dB)=-14.8368 | 84.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[362/500] train loss=0.040799, nmse(dB)=-9.2410 | val loss=0.011118, nmse(dB)=-14.8382 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[363/500] train loss=0.040630, nmse(dB)=-9.2561 | val loss=0.011119, nmse(dB)=-14.8376 | 84.8s\n",
      "[364/500] train loss=0.040653, nmse(dB)=-9.2531 | val loss=0.011121, nmse(dB)=-14.8367 | 87.6s\n",
      "[365/500] train loss=0.040692, nmse(dB)=-9.2501 | val loss=0.011119, nmse(dB)=-14.8378 | 85.4s\n",
      "[366/500] train loss=0.040607, nmse(dB)=-9.2578 | val loss=0.011120, nmse(dB)=-14.8372 | 84.2s\n",
      "[367/500] train loss=0.040750, nmse(dB)=-9.2465 | val loss=0.011121, nmse(dB)=-14.8368 | 87.4s\n",
      "[368/500] train loss=0.040738, nmse(dB)=-9.2426 | val loss=0.011120, nmse(dB)=-14.8373 | 84.9s\n",
      "[369/500] train loss=0.040606, nmse(dB)=-9.2595 | val loss=0.011119, nmse(dB)=-14.8382 | 84.5s\n",
      "[370/500] train loss=0.040660, nmse(dB)=-9.2509 | val loss=0.011116, nmse(dB)=-14.8396 | 89.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[371/500] train loss=0.040711, nmse(dB)=-9.2464 | val loss=0.011115, nmse(dB)=-14.8402 | 84.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[372/500] train loss=0.040670, nmse(dB)=-9.2481 | val loss=0.011115, nmse(dB)=-14.8403 | 87.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[373/500] train loss=0.040696, nmse(dB)=-9.2505 | val loss=0.011116, nmse(dB)=-14.8400 | 84.7s\n",
      "[374/500] train loss=0.040578, nmse(dB)=-9.2585 | val loss=0.011118, nmse(dB)=-14.8391 | 87.7s\n",
      "[375/500] train loss=0.040729, nmse(dB)=-9.2419 | val loss=0.011118, nmse(dB)=-14.8387 | 84.7s\n",
      "[376/500] train loss=0.040632, nmse(dB)=-9.2545 | val loss=0.011120, nmse(dB)=-14.8385 | 84.6s\n",
      "[377/500] train loss=0.040642, nmse(dB)=-9.2549 | val loss=0.011117, nmse(dB)=-14.8397 | 88.0s\n",
      "[378/500] train loss=0.040586, nmse(dB)=-9.2585 | val loss=0.011117, nmse(dB)=-14.8396 | 85.1s\n",
      "[379/500] train loss=0.040669, nmse(dB)=-9.2476 | val loss=0.011114, nmse(dB)=-14.8412 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[380/500] train loss=0.040715, nmse(dB)=-9.2472 | val loss=0.011113, nmse(dB)=-14.8419 | 84.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[381/500] train loss=0.040599, nmse(dB)=-9.2523 | val loss=0.011113, nmse(dB)=-14.8418 | 87.9s\n",
      "[382/500] train loss=0.040591, nmse(dB)=-9.2547 | val loss=0.011112, nmse(dB)=-14.8418 | 85.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[383/500] train loss=0.040570, nmse(dB)=-9.2564 | val loss=0.011114, nmse(dB)=-14.8410 | 86.7s\n",
      "[384/500] train loss=0.040585, nmse(dB)=-9.2546 | val loss=0.011116, nmse(dB)=-14.8404 | 84.9s\n",
      "[385/500] train loss=0.040632, nmse(dB)=-9.2540 | val loss=0.011116, nmse(dB)=-14.8407 | 86.2s\n",
      "[386/500] train loss=0.040535, nmse(dB)=-9.2589 | val loss=0.011116, nmse(dB)=-14.8404 | 87.4s\n",
      "[387/500] train loss=0.040573, nmse(dB)=-9.2555 | val loss=0.011115, nmse(dB)=-14.8410 | 84.7s\n",
      "[388/500] train loss=0.040660, nmse(dB)=-9.2480 | val loss=0.011114, nmse(dB)=-14.8419 | 84.9s\n",
      "[389/500] train loss=0.040548, nmse(dB)=-9.2559 | val loss=0.011113, nmse(dB)=-14.8420 | 88.0s\n",
      "[390/500] train loss=0.040559, nmse(dB)=-9.2558 | val loss=0.011113, nmse(dB)=-14.8422 | 84.9s\n",
      "[391/500] train loss=0.040564, nmse(dB)=-9.2547 | val loss=0.011113, nmse(dB)=-14.8422 | 87.8s\n",
      "[392/500] train loss=0.040577, nmse(dB)=-9.2541 | val loss=0.011114, nmse(dB)=-14.8416 | 84.9s\n",
      "[393/500] train loss=0.040590, nmse(dB)=-9.2504 | val loss=0.011112, nmse(dB)=-14.8426 | 87.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[394/500] train loss=0.040488, nmse(dB)=-9.2650 | val loss=0.011112, nmse(dB)=-14.8427 | 85.2s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[395/500] train loss=0.040491, nmse(dB)=-9.2649 | val loss=0.011113, nmse(dB)=-14.8421 | 87.7s\n",
      "[396/500] train loss=0.040616, nmse(dB)=-9.2498 | val loss=0.011113, nmse(dB)=-14.8419 | 84.7s\n",
      "[397/500] train loss=0.040554, nmse(dB)=-9.2567 | val loss=0.011114, nmse(dB)=-14.8412 | 84.8s\n",
      "[398/500] train loss=0.040488, nmse(dB)=-9.2633 | val loss=0.011114, nmse(dB)=-14.8416 | 87.4s\n",
      "[399/500] train loss=0.040503, nmse(dB)=-9.2647 | val loss=0.011114, nmse(dB)=-14.8415 | 84.9s\n",
      "[400/500] train loss=0.040584, nmse(dB)=-9.2547 | val loss=0.011114, nmse(dB)=-14.8414 | 87.6s\n",
      "[401/500] train loss=0.040478, nmse(dB)=-9.2603 | val loss=0.011114, nmse(dB)=-14.8415 | 84.4s\n",
      "[402/500] train loss=0.040553, nmse(dB)=-9.2563 | val loss=0.011113, nmse(dB)=-14.8419 | 87.3s\n",
      "[403/500] train loss=0.040549, nmse(dB)=-9.2570 | val loss=0.011114, nmse(dB)=-14.8416 | 84.4s\n",
      "[404/500] train loss=0.040613, nmse(dB)=-9.2504 | val loss=0.011113, nmse(dB)=-14.8420 | 87.5s\n",
      "[405/500] train loss=0.040509, nmse(dB)=-9.2594 | val loss=0.011112, nmse(dB)=-14.8430 | 84.0s\n",
      "[406/500] train loss=0.040517, nmse(dB)=-9.2629 | val loss=0.011112, nmse(dB)=-14.8430 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[407/500] train loss=0.040522, nmse(dB)=-9.2613 | val loss=0.011111, nmse(dB)=-14.8433 | 87.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[408/500] train loss=0.040500, nmse(dB)=-9.2636 | val loss=0.011111, nmse(dB)=-14.8432 | 84.5s\n",
      "[409/500] train loss=0.040506, nmse(dB)=-9.2631 | val loss=0.011112, nmse(dB)=-14.8428 | 88.1s\n",
      "[410/500] train loss=0.040490, nmse(dB)=-9.2596 | val loss=0.011112, nmse(dB)=-14.8434 | 84.4s\n",
      "[411/500] train loss=0.040515, nmse(dB)=-9.2630 | val loss=0.011111, nmse(dB)=-14.8439 | 84.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[412/500] train loss=0.040535, nmse(dB)=-9.2622 | val loss=0.011110, nmse(dB)=-14.8439 | 87.9s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[413/500] train loss=0.040480, nmse(dB)=-9.2647 | val loss=0.011110, nmse(dB)=-14.8440 | 84.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[414/500] train loss=0.040551, nmse(dB)=-9.2601 | val loss=0.011111, nmse(dB)=-14.8439 | 88.1s\n",
      "[415/500] train loss=0.040516, nmse(dB)=-9.2634 | val loss=0.011111, nmse(dB)=-14.8438 | 84.3s\n",
      "[416/500] train loss=0.040498, nmse(dB)=-9.2593 | val loss=0.011111, nmse(dB)=-14.8439 | 84.0s\n",
      "[417/500] train loss=0.040419, nmse(dB)=-9.2715 | val loss=0.011111, nmse(dB)=-14.8441 | 88.0s\n",
      "[418/500] train loss=0.040360, nmse(dB)=-9.2734 | val loss=0.011110, nmse(dB)=-14.8446 | 84.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[419/500] train loss=0.040508, nmse(dB)=-9.2587 | val loss=0.011109, nmse(dB)=-14.8448 | 87.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[420/500] train loss=0.040471, nmse(dB)=-9.2658 | val loss=0.011108, nmse(dB)=-14.8453 | 84.0s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[421/500] train loss=0.040465, nmse(dB)=-9.2631 | val loss=0.011109, nmse(dB)=-14.8452 | 84.1s\n",
      "[422/500] train loss=0.040464, nmse(dB)=-9.2643 | val loss=0.011109, nmse(dB)=-14.8451 | 87.4s\n",
      "[423/500] train loss=0.040527, nmse(dB)=-9.2576 | val loss=0.011108, nmse(dB)=-14.8453 | 84.4s\n",
      "[424/500] train loss=0.040554, nmse(dB)=-9.2519 | val loss=0.011109, nmse(dB)=-14.8449 | 84.7s\n",
      "[425/500] train loss=0.040494, nmse(dB)=-9.2628 | val loss=0.011109, nmse(dB)=-14.8449 | 87.9s\n",
      "[426/500] train loss=0.040478, nmse(dB)=-9.2657 | val loss=0.011109, nmse(dB)=-14.8450 | 84.2s\n",
      "[427/500] train loss=0.040453, nmse(dB)=-9.2634 | val loss=0.011110, nmse(dB)=-14.8448 | 88.0s\n",
      "[428/500] train loss=0.040492, nmse(dB)=-9.2652 | val loss=0.011110, nmse(dB)=-14.8447 | 84.7s\n",
      "[429/500] train loss=0.040404, nmse(dB)=-9.2699 | val loss=0.011110, nmse(dB)=-14.8448 | 84.4s\n",
      "[430/500] train loss=0.040540, nmse(dB)=-9.2559 | val loss=0.011110, nmse(dB)=-14.8449 | 88.0s\n",
      "[431/500] train loss=0.040516, nmse(dB)=-9.2592 | val loss=0.011109, nmse(dB)=-14.8450 | 84.5s\n",
      "[432/500] train loss=0.040467, nmse(dB)=-9.2632 | val loss=0.011110, nmse(dB)=-14.8449 | 88.1s\n",
      "[433/500] train loss=0.040471, nmse(dB)=-9.2653 | val loss=0.011109, nmse(dB)=-14.8451 | 84.4s\n",
      "[434/500] train loss=0.040506, nmse(dB)=-9.2583 | val loss=0.011109, nmse(dB)=-14.8451 | 84.4s\n",
      "[435/500] train loss=0.040387, nmse(dB)=-9.2687 | val loss=0.011109, nmse(dB)=-14.8451 | 88.5s\n",
      "[436/500] train loss=0.040484, nmse(dB)=-9.2628 | val loss=0.011109, nmse(dB)=-14.8455 | 84.7s\n",
      "[437/500] train loss=0.040500, nmse(dB)=-9.2634 | val loss=0.011108, nmse(dB)=-14.8459 | 87.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[438/500] train loss=0.040551, nmse(dB)=-9.2586 | val loss=0.011108, nmse(dB)=-14.8460 | 84.5s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[439/500] train loss=0.040501, nmse(dB)=-9.2573 | val loss=0.011108, nmse(dB)=-14.8459 | 84.5s\n",
      "[440/500] train loss=0.040501, nmse(dB)=-9.2592 | val loss=0.011108, nmse(dB)=-14.8460 | 88.1s\n",
      "[441/500] train loss=0.040446, nmse(dB)=-9.2645 | val loss=0.011108, nmse(dB)=-14.8458 | 84.4s\n",
      "[442/500] train loss=0.040475, nmse(dB)=-9.2604 | val loss=0.011108, nmse(dB)=-14.8458 | 88.3s\n",
      "[443/500] train loss=0.040455, nmse(dB)=-9.2653 | val loss=0.011108, nmse(dB)=-14.8457 | 84.8s\n",
      "[444/500] train loss=0.040460, nmse(dB)=-9.2674 | val loss=0.011108, nmse(dB)=-14.8458 | 84.5s\n",
      "[445/500] train loss=0.040514, nmse(dB)=-9.2595 | val loss=0.011108, nmse(dB)=-14.8457 | 87.9s\n",
      "[446/500] train loss=0.040407, nmse(dB)=-9.2695 | val loss=0.011108, nmse(dB)=-14.8459 | 84.5s\n",
      "[447/500] train loss=0.040497, nmse(dB)=-9.2606 | val loss=0.011108, nmse(dB)=-14.8459 | 88.3s\n",
      "[448/500] train loss=0.040438, nmse(dB)=-9.2703 | val loss=0.011108, nmse(dB)=-14.8460 | 84.7s\n",
      "[449/500] train loss=0.040452, nmse(dB)=-9.2657 | val loss=0.011108, nmse(dB)=-14.8459 | 84.5s\n",
      "[450/500] train loss=0.040491, nmse(dB)=-9.2601 | val loss=0.011108, nmse(dB)=-14.8459 | 87.8s\n",
      "[451/500] train loss=0.040435, nmse(dB)=-9.2728 | val loss=0.011108, nmse(dB)=-14.8459 | 84.4s\n",
      "[452/500] train loss=0.040491, nmse(dB)=-9.2577 | val loss=0.011108, nmse(dB)=-14.8459 | 88.1s\n",
      "[453/500] train loss=0.040387, nmse(dB)=-9.2683 | val loss=0.011108, nmse(dB)=-14.8459 | 84.7s\n",
      "[454/500] train loss=0.040474, nmse(dB)=-9.2595 | val loss=0.011108, nmse(dB)=-14.8461 | 84.5s\n",
      "[455/500] train loss=0.040411, nmse(dB)=-9.2689 | val loss=0.011108, nmse(dB)=-14.8461 | 88.1s\n",
      "[456/500] train loss=0.040466, nmse(dB)=-9.2640 | val loss=0.011108, nmse(dB)=-14.8461 | 84.4s\n",
      "[457/500] train loss=0.040435, nmse(dB)=-9.2712 | val loss=0.011108, nmse(dB)=-14.8461 | 88.2s\n",
      "[458/500] train loss=0.040449, nmse(dB)=-9.2654 | val loss=0.011108, nmse(dB)=-14.8461 | 84.7s\n",
      "[459/500] train loss=0.040388, nmse(dB)=-9.2702 | val loss=0.011108, nmse(dB)=-14.8460 | 84.9s\n",
      "[460/500] train loss=0.040419, nmse(dB)=-9.2649 | val loss=0.011108, nmse(dB)=-14.8459 | 88.3s\n",
      "[461/500] train loss=0.040427, nmse(dB)=-9.2695 | val loss=0.011108, nmse(dB)=-14.8460 | 84.7s\n",
      "[462/500] train loss=0.040572, nmse(dB)=-9.2555 | val loss=0.011108, nmse(dB)=-14.8459 | 88.3s\n",
      "[463/500] train loss=0.040536, nmse(dB)=-9.2589 | val loss=0.011108, nmse(dB)=-14.8460 | 84.4s\n",
      "[464/500] train loss=0.040414, nmse(dB)=-9.2689 | val loss=0.011108, nmse(dB)=-14.8461 | 84.4s\n",
      "[465/500] train loss=0.040450, nmse(dB)=-9.2632 | val loss=0.011108, nmse(dB)=-14.8461 | 88.3s\n",
      "[466/500] train loss=0.040391, nmse(dB)=-9.2672 | val loss=0.011108, nmse(dB)=-14.8461 | 84.4s\n",
      "[467/500] train loss=0.040532, nmse(dB)=-9.2576 | val loss=0.011108, nmse(dB)=-14.8462 | 88.3s\n",
      "[468/500] train loss=0.040479, nmse(dB)=-9.2597 | val loss=0.011108, nmse(dB)=-14.8463 | 84.4s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[469/500] train loss=0.040474, nmse(dB)=-9.2636 | val loss=0.011107, nmse(dB)=-14.8463 | 84.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[470/500] train loss=0.040443, nmse(dB)=-9.2666 | val loss=0.011107, nmse(dB)=-14.8464 | 88.8s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[471/500] train loss=0.040479, nmse(dB)=-9.2622 | val loss=0.011107, nmse(dB)=-14.8464 | 84.6s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[472/500] train loss=0.040449, nmse(dB)=-9.2690 | val loss=0.011107, nmse(dB)=-14.8464 | 88.1s\n",
      "[473/500] train loss=0.040450, nmse(dB)=-9.2616 | val loss=0.011107, nmse(dB)=-14.8464 | 84.9s\n",
      "[474/500] train loss=0.040495, nmse(dB)=-9.2605 | val loss=0.011107, nmse(dB)=-14.8464 | 84.7s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[475/500] train loss=0.040444, nmse(dB)=-9.2654 | val loss=0.011107, nmse(dB)=-14.8464 | 88.1s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[476/500] train loss=0.040483, nmse(dB)=-9.2612 | val loss=0.011107, nmse(dB)=-14.8464 | 84.7s\n",
      "[477/500] train loss=0.040426, nmse(dB)=-9.2699 | val loss=0.011107, nmse(dB)=-14.8464 | 88.3s\n",
      "[478/500] train loss=0.040453, nmse(dB)=-9.2643 | val loss=0.011107, nmse(dB)=-14.8464 | 84.5s\n",
      "[479/500] train loss=0.040492, nmse(dB)=-9.2638 | val loss=0.011107, nmse(dB)=-14.8464 | 84.7s\n",
      "[480/500] train loss=0.040378, nmse(dB)=-9.2693 | val loss=0.011107, nmse(dB)=-14.8465 | 88.3s\n",
      "  ↳ saved best_convlstm_earlyfusion.pt (best val loss)\n",
      "[481/500] train loss=0.040417, nmse(dB)=-9.2701 | val loss=0.011107, nmse(dB)=-14.8465 | 84.7s\n",
      "[482/500] train loss=0.040360, nmse(dB)=-9.2701 | val loss=0.011107, nmse(dB)=-14.8464 | 88.3s\n",
      "[483/500] train loss=0.040441, nmse(dB)=-9.2624 | val loss=0.011107, nmse(dB)=-14.8464 | 84.5s\n",
      "[484/500] train loss=0.040496, nmse(dB)=-9.2625 | val loss=0.011107, nmse(dB)=-14.8464 | 84.5s\n",
      "[485/500] train loss=0.040480, nmse(dB)=-9.2625 | val loss=0.011107, nmse(dB)=-14.8464 | 88.2s\n",
      "[486/500] train loss=0.040368, nmse(dB)=-9.2735 | val loss=0.011107, nmse(dB)=-14.8464 | 84.3s\n",
      "[487/500] train loss=0.040407, nmse(dB)=-9.2690 | val loss=0.011107, nmse(dB)=-14.8465 | 88.3s\n",
      "[488/500] train loss=0.040452, nmse(dB)=-9.2667 | val loss=0.011107, nmse(dB)=-14.8464 | 84.5s\n",
      "[489/500] train loss=0.040514, nmse(dB)=-9.2603 | val loss=0.011107, nmse(dB)=-14.8464 | 84.9s\n",
      "[490/500] train loss=0.040327, nmse(dB)=-9.2735 | val loss=0.011107, nmse(dB)=-14.8464 | 88.1s\n",
      "[491/500] train loss=0.040425, nmse(dB)=-9.2693 | val loss=0.011107, nmse(dB)=-14.8464 | 84.4s\n",
      "[492/500] train loss=0.040439, nmse(dB)=-9.2650 | val loss=0.011107, nmse(dB)=-14.8464 | 88.1s\n",
      "[493/500] train loss=0.040401, nmse(dB)=-9.2716 | val loss=0.011107, nmse(dB)=-14.8464 | 84.4s\n",
      "[494/500] train loss=0.040451, nmse(dB)=-9.2620 | val loss=0.011107, nmse(dB)=-14.8464 | 84.4s\n",
      "[495/500] train loss=0.040431, nmse(dB)=-9.2645 | val loss=0.011107, nmse(dB)=-14.8464 | 88.4s\n",
      "[496/500] train loss=0.040365, nmse(dB)=-9.2741 | val loss=0.011107, nmse(dB)=-14.8464 | 84.4s\n",
      "[497/500] train loss=0.040470, nmse(dB)=-9.2624 | val loss=0.011107, nmse(dB)=-14.8464 | 88.2s\n",
      "[498/500] train loss=0.040397, nmse(dB)=-9.2644 | val loss=0.011107, nmse(dB)=-14.8464 | 84.8s\n",
      "[499/500] train loss=0.040544, nmse(dB)=-9.2548 | val loss=0.011107, nmse(dB)=-14.8464 | 84.8s\n",
      "[500/500] train loss=0.040465, nmse(dB)=-9.2624 | val loss=0.011107, nmse(dB)=-14.8464 | 87.8s\n",
      "\n",
      "TOTAL TRAIN TIME: 717.06 min (43023.6 sec)\n",
      "BEST (by val loss) -> val loss=0.011107, val nmse(dB)=-14.8465\n",
      "\n",
      "=== Done ===\n",
      "Best val loss      : 0.011107\n",
      "Best val NMSE (dB) : -14.8465\n",
      "Total train time   : 717.06 min (43023.6 sec)\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "model_convlstm = ConvLSTM_EarlyFusion_Forecaster(\n",
    "    F_in=F_in,\n",
    "    F_out=F_out,\n",
    "    img_map_ch=64,\n",
    "    ch_map_ch=16,\n",
    "    map_hw=14,\n",
    "    convlstm_hidden=64,\n",
    "    convlstm_layers=2,\n",
    "    convlstm_kernel=3,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(\"\\n=== Training: ConvLSTM Early Fusion ===\")\n",
    "best_loss, best_nmse_db, total_time_sec = fit_model(\n",
    "    model=model_convlstm,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    ckpt_path=\"best_convlstm_earlyfusion.pt\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== Done ===\")\n",
    "print(f\"Best val loss      : {best_loss:.6f}\")\n",
    "print(f\"Best val NMSE (dB) : {best_nmse_db:.4f}\")\n",
    "print(f\"Total train time   : {total_time_sec/60:.2f} min ({total_time_sec:.1f} sec)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23882e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y abs mean: 0.479672908782959\n",
      "y abs max : 1.0\n",
      "y power   : 0.2711747884750366\n",
      "yhat abs mean: 0.48492735624313354\n",
      "yhat abs max : 0.6587302088737488\n",
      "yhat power   : 0.23690596222877502\n"
     ]
    }
   ],
   "source": [
    "ch, img, y = next(iter(train_loader))\n",
    "\n",
    "# 안전하게 device 이동 (dataset이 이미 cuda를 주더라도 문제 없음)\n",
    "ch = ch.to(device, non_blocking=True)\n",
    "img = img.to(device, non_blocking=True)\n",
    "y  = y.to(device, non_blocking=True)\n",
    "\n",
    "print(\"y abs mean:\", y.abs().mean().item())\n",
    "print(\"y abs max :\", y.abs().max().item())\n",
    "print(\"y power   :\", (y**2).mean().item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_convlstm.eval()\n",
    "    yhat = model_convlstm(ch, img)\n",
    "\n",
    "print(\"yhat abs mean:\", yhat.abs().mean().item())\n",
    "print(\"yhat abs max :\", yhat.abs().max().item())\n",
    "print(\"yhat power   :\", (yhat**2).mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a090a4",
   "metadata": {},
   "source": [
    "# 데이터 입력 및 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb262af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def debug_batch_and_forward(loader, device, model=None, name=\"ConvLSTM\", force_img_seq=True):\n",
    "    \"\"\"\n",
    "    - loader에서 배치 하나 뽑아서 shape/device/dtype 확인\n",
    "    - model이 있으면 forward까지 해서 yhat shape + 간단 통계 확인\n",
    "    - ConvLSTM은 기본적으로 img=(B,T,3,H,W)를 기대\n",
    "      -> img가 (B,3,H,W)면 force_img_seq=True일 때 (B,T,3,H,W)로 자동 확장\n",
    "    \"\"\"\n",
    "    ch, img, y = next(iter(loader))\n",
    "\n",
    "    def info(x, label):\n",
    "        print(f\"{label:>4}: shape={tuple(x.shape)} dtype={x.dtype} device={x.device}\")\n",
    "\n",
    "    print(\"\\n=== one batch tensor info ===\")\n",
    "    info(ch, \"ch\")\n",
    "    info(img, \"img\")\n",
    "    info(y,  \"y\")\n",
    "\n",
    "    # img 차원 설명 + ConvLSTM용 자동 확장\n",
    "    if img.dim() == 5:\n",
    "        print(\"img format: (B,T,3,H,W) ✅\")\n",
    "    elif img.dim() == 4:\n",
    "        print(\"img format: (B,3,H,W)\")\n",
    "        if force_img_seq and model is not None:\n",
    "            # ch에서 T를 가져와 img를 (B,T,3,H,W)로 맞춤\n",
    "            T = ch.shape[1]\n",
    "            img = img.unsqueeze(1).repeat(1, T, 1, 1, 1)\n",
    "            print(f\" -> expanded img to (B,T,3,H,W) with T={T}\")\n",
    "            info(img, \"img*\")\n",
    "    else:\n",
    "        print(f\"img format: unexpected dim={img.dim()}\")\n",
    "\n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "        ch = ch.to(device, non_blocking=True)\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        y  = y.to(device, non_blocking=True)\n",
    "\n",
    "        yhat = model(ch, img)\n",
    "        info(yhat, f\"{name}_yhat\")\n",
    "\n",
    "        # 간단 통계\n",
    "        print(f\"{name} yhat abs mean:\", yhat.abs().mean().item())\n",
    "        print(f\"{name} yhat abs max :\", yhat.abs().max().item())\n",
    "        print(f\"{name} yhat power   :\", (yhat**2).mean().item())\n",
    "\n",
    "        return (ch, img, y, yhat)\n",
    "\n",
    "    return (ch, img, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c47fcab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== dataset sizes ===\n",
      "N(comm_frames): 100\n",
      "N(cam_files)  : 7012\n",
      "N(min)        : 100\n",
      "past_len      : 16\n",
      "len(ds)       : 84\n",
      "len(train_ds) : 63\n",
      "len(val_ds)   : 21\n",
      "len(train_loader): 2\n",
      "len(val_loader)  : 1\n",
      "\n",
      "=== one batch tensor info ===\n",
      "  ch: shape=(32, 16, 2048) dtype=torch.float32 device=cuda:0\n",
      " img: shape=(32, 16, 3, 224, 224) dtype=torch.float32 device=cuda:0\n",
      "   y: shape=(32, 2048) dtype=torch.float32 device=cuda:0\n",
      "img format: (B,T,3,H,W) ✅\n",
      "ConvLSTM_yhat: shape=(32, 2048) dtype=torch.float32 device=cuda:0\n",
      "ConvLSTM yhat abs mean: 0.4949188828468323\n",
      "ConvLSTM yhat abs max : 0.7043417096138\n",
      "ConvLSTM yhat power   : 0.24614623188972473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.7071, 0.7267, 0.7437,  ..., 0.3161, 0.2963, 0.2776],\n",
       "          [0.3173, 0.2997, 0.2824,  ..., 0.9147, 0.9237, 0.9295],\n",
       "          [0.9300, 0.9405, 0.9473,  ..., 0.3734, 0.3941, 0.4135],\n",
       "          ...,\n",
       "          [0.1897, 0.1977, 0.2072,  ..., 0.2411, 0.2543, 0.2662],\n",
       "          [0.6055, 0.6269, 0.6474,  ..., 0.7570, 0.7697, 0.7793],\n",
       "          [0.6364, 0.6241, 0.6110,  ..., 0.3712, 0.3637, 0.3543]],\n",
       " \n",
       "         [[0.5360, 0.5395, 0.5451,  ..., 0.4707, 0.4819, 0.4974],\n",
       "          [0.6890, 0.6798, 0.6731,  ..., 0.6881, 0.6868, 0.6869],\n",
       "          [0.5834, 0.5985, 0.6148,  ..., 0.7316, 0.7507, 0.7668],\n",
       "          ...,\n",
       "          [0.3330, 0.3232, 0.3142,  ..., 0.7128, 0.6953, 0.6739],\n",
       "          [0.6469, 0.6474, 0.6486,  ..., 0.5114, 0.5193, 0.5300],\n",
       "          [0.6921, 0.6864, 0.6767,  ..., 0.5602, 0.5750, 0.5904]],\n",
       " \n",
       "         [[0.6039, 0.5822, 0.5604,  ..., 0.2329, 0.2328, 0.2335],\n",
       "          [0.7136, 0.7239, 0.7306,  ..., 0.4165, 0.4095, 0.4035],\n",
       "          [0.7090, 0.7159, 0.7231,  ..., 0.3206, 0.3072, 0.2977],\n",
       "          ...,\n",
       "          [0.2949, 0.3053, 0.3183,  ..., 0.3187, 0.3185, 0.3202],\n",
       "          [0.3403, 0.3433, 0.3501,  ..., 0.3472, 0.3572, 0.3694],\n",
       "          [0.6202, 0.6388, 0.6552,  ..., 0.6224, 0.6280, 0.6362]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.1897, 0.1977, 0.2072,  ..., 0.2411, 0.2543, 0.2662],\n",
       "          [0.6055, 0.6269, 0.6474,  ..., 0.7570, 0.7697, 0.7793],\n",
       "          [0.6364, 0.6241, 0.6110,  ..., 0.3712, 0.3637, 0.3543],\n",
       "          ...,\n",
       "          [0.5360, 0.5395, 0.5451,  ..., 0.4707, 0.4819, 0.4974],\n",
       "          [0.6890, 0.6798, 0.6731,  ..., 0.6881, 0.6868, 0.6869],\n",
       "          [0.5834, 0.5985, 0.6148,  ..., 0.7316, 0.7507, 0.7668]],\n",
       " \n",
       "         [[0.5834, 0.5985, 0.6148,  ..., 0.7316, 0.7507, 0.7668],\n",
       "          [0.6920, 0.6924, 0.6915,  ..., 0.5997, 0.5878, 0.5722],\n",
       "          [0.6329, 0.6213, 0.6122,  ..., 0.4569, 0.4423, 0.4281],\n",
       "          ...,\n",
       "          [0.6921, 0.6864, 0.6767,  ..., 0.5602, 0.5750, 0.5904],\n",
       "          [0.3645, 0.3470, 0.3292,  ..., 0.5890, 0.5795, 0.5668],\n",
       "          [0.4893, 0.5056, 0.5242,  ..., 0.3959, 0.4013, 0.4057]],\n",
       " \n",
       "         [[0.7136, 0.7239, 0.7306,  ..., 0.4165, 0.4095, 0.4035],\n",
       "          [0.7090, 0.7159, 0.7231,  ..., 0.3206, 0.3072, 0.2977],\n",
       "          [0.6462, 0.6325, 0.6184,  ..., 0.3952, 0.4065, 0.4158],\n",
       "          ...,\n",
       "          [0.3403, 0.3433, 0.3501,  ..., 0.3472, 0.3572, 0.3694],\n",
       "          [0.6202, 0.6388, 0.6552,  ..., 0.6224, 0.6280, 0.6362],\n",
       "          [0.4360, 0.4483, 0.4610,  ..., 0.5335, 0.5439, 0.5541]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]]],\n",
       " \n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          ...,\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]],\n",
       " \n",
       " \n",
       "          [[[ 0.5193,  0.5355,  0.5059,  ...,  0.7082,  0.7082,  0.7117],\n",
       "            [ 0.4581,  0.4724,  0.4683,  ...,  0.7082,  0.7203,  0.7554],\n",
       "            [ 0.4357,  0.4166,  0.4078,  ...,  0.6734,  0.6966,  0.7174],\n",
       "            ...,\n",
       "            [ 0.9458,  0.9111,  0.9469,  ...,  0.1478,  0.1915,  0.2846],\n",
       "            [ 0.9205,  0.9474,  0.9306,  ..., -0.1110,  0.0209,  0.3159],\n",
       "            [ 0.9646,  1.0225,  0.9646,  ..., -0.0182,  0.1729,  0.3285]],\n",
       " \n",
       "           [[ 0.9055,  0.9220,  0.9067,  ...,  1.0311,  1.0455,  1.0333],\n",
       "            [ 0.8732,  0.8833,  0.8870,  ...,  1.0333,  1.0409,  1.0630],\n",
       "            [ 0.8550,  0.8529,  0.8529,  ...,  1.0268,  1.0280,  1.0440],\n",
       "            ...,\n",
       "            [ 1.1506,  1.1309,  1.1596,  ...,  0.2716,  0.3217,  0.4168],\n",
       "            [ 1.1395,  1.1664,  1.1506,  ..., -0.0272,  0.1415,  0.4902],\n",
       "            [ 1.1840,  1.2289,  1.1708,  ...,  0.0421,  0.2882,  0.5040]],\n",
       " \n",
       "           [[ 1.2946,  1.2970,  1.2832,  ...,  1.3867,  1.3851,  1.3888],\n",
       "            [ 1.2805,  1.2851,  1.2805,  ...,  1.3860,  1.3851,  1.3888],\n",
       "            [ 1.2652,  1.2631,  1.2631,  ...,  1.3844,  1.3817,  1.3836],\n",
       "            ...,\n",
       "            [ 1.4374,  1.4128,  1.4463,  ...,  0.4403,  0.4841,  0.6233],\n",
       "            [ 1.4236,  1.4531,  1.4365,  ...,  0.1637,  0.3200,  0.6552],\n",
       "            [ 1.4582,  1.5143,  1.4561,  ...,  0.2293,  0.4579,  0.6540]]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0.5688, 0.5617, 0.5533,  ..., 0.2597, 0.2638, 0.2657],\n",
       "         [0.3645, 0.3470, 0.3292,  ..., 0.5890, 0.5795, 0.5668],\n",
       "         [0.4360, 0.4483, 0.4610,  ..., 0.5335, 0.5439, 0.5541],\n",
       "         ...,\n",
       "         [0.6920, 0.6924, 0.6915,  ..., 0.5997, 0.5878, 0.5722],\n",
       "         [0.5551, 0.5468, 0.5363,  ..., 0.6562, 0.6462, 0.6375],\n",
       "         [0.3288, 0.3294, 0.3337,  ..., 0.3518, 0.3597, 0.3662]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0.5335, 0.5295, 0.5428,  ..., 0.5122, 0.4987, 0.5039],\n",
       "         [0.5403, 0.5458, 0.5473,  ..., 0.5199, 0.5182, 0.5124],\n",
       "         [0.5403, 0.5458, 0.5472,  ..., 0.5188, 0.5178, 0.5120],\n",
       "         ...,\n",
       "         [0.5407, 0.5464, 0.5474,  ..., 0.5177, 0.5183, 0.5118],\n",
       "         [0.5399, 0.5448, 0.5470,  ..., 0.5181, 0.5166, 0.5114],\n",
       "         [0.5403, 0.5458, 0.5472,  ..., 0.5191, 0.5179, 0.5121]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== dataset sizes ===\")\n",
    "print(\"N(comm_frames):\", len(comm_frames))\n",
    "print(\"N(cam_files)  :\", len(cam_files))\n",
    "print(\"N(min)        :\", min(len(comm_frames), len(cam_files)))\n",
    "print(\"past_len      :\", ds.past_len)\n",
    "print(\"len(ds)       :\", len(ds))\n",
    "print(\"len(train_ds) :\", len(train_ds))\n",
    "print(\"len(val_ds)   :\", len(val_ds))\n",
    "print(\"len(train_loader):\", len(train_loader))\n",
    "print(\"len(val_loader)  :\", len(val_loader))\n",
    "\n",
    "# ConvLSTM\n",
    "debug_batch_and_forward(train_loader, device, model_convlstm, name=\"ConvLSTM\", force_img_seq=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097dbcc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
