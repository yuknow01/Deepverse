{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b025bdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (657968115.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfind /mnt/ssd_7t_2/yuknow01/Deepverse/scenarios/DT31 -type f | wc -l\u001b[39m\n                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cdba8",
   "metadata": {},
   "source": [
    "#  Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as  np\n",
    "from lwm_multi_model import multi_modal_lwm  # 클래스 import\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader, Subset\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from deepverse import ParameterManager\n",
    "from deepverse.scenario import ScenarioManager\n",
    "from deepverse import Dataset\n",
    "\n",
    "from deepverse.visualizers import ImageVisualizer, LidarVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babd3da",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 중인 장치: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Scenes 2000\n",
    "## Subcarriers 64\n",
    "\n",
    "scenarios_name = \"DT31\"\n",
    "config_path = f\"scenarios/{scenarios_name}/param/config.m\"\n",
    "param_manager = ParameterManager(config_path)\n",
    "\n",
    "params = param_manager.get_params()\n",
    "\n",
    "param_manager.params[\"scenes\"] =list(range(100))\n",
    "param_manager.params[\"comm\"][\"OFDM\"][\"selected_subcarriers\"] = list(range(64))\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"현재 사용 중인 장치:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d7ca8",
   "metadata": {},
   "source": [
    "# Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d341a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating camera dataset: ⏳ In progress\n",
      "\u001b[F\u001b[KGenerating camera dataset: ✅ Completed (0.00s)\n",
      "Generating LiDAR dataset: ⏳ In progress\n",
      "\u001b[F\u001b[KGenerating LiDAR dataset: ✅ Completed (0.00s)\n",
      "Generating mobility dataset: ⏳ In progress\n",
      "\u001b[F\u001b[KGenerating mobility dataset: ✅ Completed (0.00s)\n",
      "Generating comm dataset: ⏳ In progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/ssd_7t_2/Deepverse/scenarios/DT31/wireless/scene_0/BS1_BS.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/scipy/io/matlab/_mio.py:39\u001b[39m, in \u001b[36m_open_file\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/ssd_7t_2/Deepverse/scenarios/DT31/wireless/scene_0/BS1_BS.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/datasets/dataset.py:74\u001b[39m, in \u001b[36mDataset.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     72\u001b[39m tqdm.write(\u001b[33m\"\u001b[39m\u001b[33mGenerating comm dataset: ⏳ In progress\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m start_time = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28mself\u001b[39m.comm_dataset = \u001b[43mCommunicationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_filtered_params\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcomm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m end_time = time.perf_counter()\n\u001b[32m     76\u001b[39m tqdm.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[F\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[KGenerating comm dataset: ✅ Completed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time-start_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/datasets/wireless_datasets.py:93\u001b[39m, in \u001b[36mCommunicationDataset.__init__\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m.params = params\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_parameters(\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPARAMSET_DYNAMIC_SCENES\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/datasets/wireless_datasets.py:125\u001b[39m, in \u001b[36mCommunicationDataset._generate_data\u001b[39m\u001b[34m(self, scenes)\u001b[39m\n\u001b[32m    123\u001b[39m dataset = []\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, scene_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(scenes, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing Scenes\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     dataset.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_scene_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscene_idx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/datasets/wireless_datasets.py:150\u001b[39m, in \u001b[36mCommunicationDataset._generate_scene_data\u001b[39m\u001b[34m(self, scene_idx)\u001b[39m\n\u001b[32m    145\u001b[39m bs_indx = params[c.PARAMSET_ACTIVE_BS]\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m#%%\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# TODO: When adding the feature for static users, fix None for rx_idx\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# rx_idx=None to generate all users\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m raydata, bs_data[\u001b[33m'\u001b[39m\u001b[33mbs_loc\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mrt_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs_indx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrx_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m ue_channels = []\n\u001b[32m    153\u001b[39m n_ue = \u001b[38;5;28mlen\u001b[39m(raydata[\u001b[33m'\u001b[39m\u001b[33mpaths\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/wireless/RayTracingLoader.py:25\u001b[39m, in \u001b[36mRayTracingLoader.load_data\u001b[39m\u001b[34m(self, tx_idx, rx_idx, user)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rx_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     24\u001b[39m     rx_idx = np.arange(\u001b[38;5;28mself\u001b[39m.num_ue)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m ray_data, rx_locs, tx_loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_ray_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtx_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mgeneration_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrx_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_tables\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m path_list = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ray_data)), desc=\u001b[33m'\u001b[39m\u001b[33mReading ray-tracing\u001b[39m\u001b[33m'\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/wireless/RayTracingLoader.py:84\u001b[39m, in \u001b[36mRayTracingLoader._load_ray_data\u001b[39m\u001b[34m(self, bs_id, generation_idx, df)\u001b[39m\n\u001b[32m     82\u001b[39m ray_data = np.array(ray_data)\n\u001b[32m     83\u001b[39m rx_locs = np.array(rx_locs)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m tx_loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_tx_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Sth better here would be good\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ray_data, rx_locs, tx_loc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/deepverse/wireless/RayTracingLoader.py:95\u001b[39m, in \u001b[36mRayTracingLoader._get_tx_location\u001b[39m\u001b[34m(self, file_data, bs_id)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# If tx location is not available in the data\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# pull it from the BS-BS file\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     94\u001b[39m     file = os.path.join(\u001b[38;5;28mself\u001b[39m.directory, \u001b[33m'\u001b[39m\u001b[33mBS\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m_BS.mat\u001b[39m\u001b[33m'\u001b[39m%(bs_id+\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     file_data = \u001b[43mscipy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     tx_loc = file_data[\u001b[33m'\u001b[39m\u001b[33mrx_locs\u001b[39m\u001b[33m'\u001b[39m][bs_id][:\u001b[32m3\u001b[39m]\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tx_loc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/scipy/io/matlab/_mio.py:234\u001b[39m, in \u001b[36mloadmat\u001b[39m\u001b[34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03mLoad MATLAB file.\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m \u001b[33;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    233\u001b[39m variable_names = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mvariable_names\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_reader_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatfile_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/scipy/io/matlab/_mio.py:17\u001b[39m, in \u001b[36m_open_file_context\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_context\u001b[39m(file_like, appendmat, mode=\u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     f, opened = \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/hoyun_312/lib/python3.12/site-packages/scipy/io/matlab/_mio.py:45\u001b[39m, in \u001b[36m_open_file\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like.endswith(\u001b[33m'\u001b[39m\u001b[33m.mat\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     44\u001b[39m         file_like += \u001b[33m'\u001b[39m\u001b[33m.mat\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m     48\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mReader needs file name or open file-like object\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     49\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/ssd_7t_2/Deepverse/scenarios/DT31/wireless/scene_0/BS1_BS.mat'"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(param_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0fe9a",
   "metadata": {},
   "source": [
    "# location dataset \n",
    " 지금 실험에서는 안쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comm = dataset.comm_dataset\n",
    "# location =  comm\n",
    "\n",
    "# location = [\n",
    "#     {\n",
    "#         \"bs_loc\": d[\"bs_loc\"],                      # (3,)\n",
    "#         \"ue_loc\": np.asarray(d[\"ue_loc\"]).squeeze() # (3,)  (원래 (1,3)이면 squeeze)\n",
    "#     }\n",
    "#     for row in comm.data      # row: [dict] 형태\n",
    "#     for d in row              # d: dict\n",
    "# ]\n",
    "\n",
    "# print(ue_location)  #)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4fe58",
   "metadata": {},
   "source": [
    "# communication  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715162f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "# UE 정보\n",
    "comm = dataset.comm_dataset\n",
    "ch = comm.data[0][0]['ue'][0]\n",
    "print(ch.coeffs.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0f7be",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e21952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeffs_from_frame(frame, ue_idx=0):\n",
    "    ue_obj = frame[\"ue\"]\n",
    "\n",
    "    # 케이스1) list/tuple이면 ue_idx로 선택\n",
    "    if isinstance(ue_obj, (list, tuple)):\n",
    "        ch_obj = ue_obj[ue_idx]\n",
    "    else:\n",
    "        # 케이스2) 단일 OFDMChannel이면 그대로 사용\n",
    "        ch_obj = ue_obj\n",
    "\n",
    "    # coeffs는 dict key가 아니라 attribute일 확률이 매우 큼\n",
    "    if hasattr(ch_obj, \"coeffs\"):\n",
    "        return ch_obj.coeffs\n",
    "\n",
    "    # 혹시 dict라면 마지막 보험\n",
    "    if isinstance(ch_obj, dict) and \"coeffs\" in ch_obj:\n",
    "        return ch_obj[\"coeffs\"]\n",
    "\n",
    "    raise TypeError(f\"Cannot get coeffs. ue type={type(ue_obj)}, ch type={type(ch_obj)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_min_max_realimag(frames, train_idx, us_idx=0):\n",
    "\n",
    "    rmin, rmax =  float('inf'), float('-inf')\n",
    "    imin, imax =  float('inf'), float('-inf')\n",
    "\n",
    "    print(\"Calculating min/max over training set...\")\n",
    "\n",
    "    for t  in train_idx:\n",
    "        frame  = frames[t]\n",
    "        cooeffs  = get_coeffs_from_frame(frame, us_idx)  # (N_subcarriers, )\n",
    "\n",
    "        rmin = min(rmin, float(cooeffs.real.min()))\n",
    "        rmax = max(rmax, float(cooeffs.real.max()))\n",
    "        imin = min(imin, float(cooeffs.imag.min()))\n",
    "        imax = max(imax, float(cooeffs.imag.max()))\n",
    "\n",
    "    print(f\"Done. rmin={rmin}, rmax={rmax}, imin={imin}, imax={imax}\")\n",
    "    return (rmin, rmax), (imin, imax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bebdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_channel_coeffs_minmax(coeffs_np, r_min, r_max, i_min, i_max, device=device, eps=1e-16):\n",
    "    # Convert Numpy to Tensor\n",
    "    coeffs = torch.from_numpy(coeffs_np).to(torch.complex128)\n",
    "    \n",
    "    r = coeffs.real\n",
    "    i = coeffs.imag\n",
    "    \n",
    "    # Min-Max Scaling [0, 1]\n",
    "    # Add eps to denominator to prevent division by zero\n",
    "    r_scaled = (r - r_min) / max(r_max - r_min, eps)\n",
    "    i_scaled = (i - i_min) / max(i_max - i_min, eps)\n",
    "    \n",
    "    # Concat (Maintains shape like (..., 2*subcarriers))\n",
    "    H = torch.cat([r_scaled, i_scaled], dim=-1).to(device)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639969a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "# 사용예시\n",
    "H = preprocess_channel_coeffs_minmax(ch.coeffs, r_min=-0.5, r_max=0.5, i_min=-0.5, i_max=0.5)\n",
    "print(H.shape)  # (1, 16, 128) 64 subcar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f94e",
   "metadata": {},
   "source": [
    "### image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef4385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: scenarios/DT31/RGB_images/unit1_cam1/0.png\n",
      "PIL size (W,H): (1920, 1080)\n",
      "np shape: (1080, 1920, 3) dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "sensor = dataset.camera_dataset.sensors[\"unit1_cam1\"]\n",
    "path0 = sensor.files[0]\n",
    "img = Image.open(path0).convert(\"RGB\")\n",
    "arr = np.array(img)\n",
    "\n",
    "print(\"path:\", path0)\n",
    "print(\"PIL size (W,H):\", img.size)\n",
    "print(\"np shape:\", arr.shape, \"dtype:\", arr.dtype)  # 보통 (H,W,3), uint8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b56a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def preprocess_img(path, img_size=IMG_SIZE, device=device):\n",
    "    # 1) load (H,W,3) uint8\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "\n",
    "    # 2) numpy -> torch, (3,H,W), float32\n",
    "    x = torch.from_numpy(arr).permute(2, 0, 1).contiguous().double()  # (3,H,W), double for ImageNet stats\n",
    "    x = x / 255.0  # [0,1]\n",
    "\n",
    "    # 3) add batch dim -> (1,3,H,W)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # 4) resize -> (1,3,224,224)\n",
    "    x = F.interpolate(x, size=(img_size, img_size),\n",
    "                      mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # 5) normalize (ImageNet)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float64).view(1, 3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float64).view(1, 3, 1, 1)\n",
    "    x = (x - mean) / std\n",
    "\n",
    "    # 6) move to device (GPU)\n",
    "    x = x.to(device, non_blocking=True)\n",
    "\n",
    "    return x  # (1,3,224,224) on device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) cuda:0\n",
      "scenarios/DT31/RGB_images/unit1_cam1/0.png\n"
     ]
    }
   ],
   "source": [
    "# 사용예시\n",
    "cd = dataset.camera_dataset\n",
    "sensor = cd.sensors['unit1_cam1']\n",
    "path0 = sensor.files[0]\n",
    "img = preprocess_img(path0, device=device)\n",
    "print(img.shape, img.device)  # torch.Size([1,3,224,224]) cuda:0\n",
    "print(path0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a4765",
   "metadata": {},
   "source": [
    "# Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comm_frames(comm):\n",
    "    frames = []\n",
    "    for row in comm.data:\n",
    "        for d in row:\n",
    "            frames.append(d)\n",
    "    return frames\n",
    "\n",
    "class MultiModalNextStepDatasetGPU(TorchDataset):\n",
    "    def __init__(self, comm_frames, cam_files, ue_idx=0, past_len=16, device=device,\n",
    "                 # Arguments for statistical values (initialized with default values)\n",
    "                 r_min=0.0, r_max=1.0, i_min=0.0, i_max=1.0):\n",
    "        \n",
    "        self.comm_frames = comm_frames\n",
    "        self.cam_files = list(cam_files)\n",
    "        self.ue_idx = ue_idx\n",
    "        self.past_len = past_len\n",
    "        self.device = device\n",
    "        \n",
    "        # Save statistical values\n",
    "        self.r_min, self.r_max = r_min, r_max\n",
    "        self.i_min, self.i_max = i_min, i_max\n",
    "\n",
    "        self.N = min(len(self.comm_frames), len(self.cam_files))\n",
    "        self.valid_start = past_len - 1\n",
    "        self.valid_end = self.N - 2 \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.valid_end - self.valid_start + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.valid_start + idx\n",
    "\n",
    "        # 1. Image Past (Apply Preprocessing)\n",
    "        img_list = []\n",
    "        for k in range(t - self.past_len + 1, t + 1):\n",
    "            img_path = self.cam_files[k]\n",
    "            img_k = preprocess_img(img_path, device=self.device).squeeze(0)\n",
    "            img_list.append(img_k)\n",
    "        img = torch.stack(img_list, dim=0)  # Shape: (past_len\n",
    "\n",
    "        \n",
    "        # 2. Channel Past (Apply Scaling)\n",
    "        ch_list = []\n",
    "        for k in range(t - self.past_len + 1, t + 1):\n",
    "            coeffs_np = get_coeffs_from_frame(self.comm_frames[k], ue_idx=self.ue_idx)\n",
    "            # Use the newly defined Min-Max preprocessing function\n",
    "            h = preprocess_channel_coeffs_minmax(\n",
    "                coeffs_np, \n",
    "                self.r_min, self.r_max, self.i_min, self.i_max, \n",
    "                device=self.device\n",
    "            ).reshape(-1)\n",
    "            ch_list.append(h)\n",
    "        channel_past = torch.stack(ch_list, dim=0)\n",
    "\n",
    "        # 3. Target (Apply Scaling) - Target must also be scaled for model training!\n",
    "        coeffs_np_next = get_coeffs_from_frame(self.comm_frames[t + 1], ue_idx=self.ue_idx)\n",
    "        target = preprocess_channel_coeffs_minmax(\n",
    "            coeffs_np_next, \n",
    "            self.r_min, self.r_max, self.i_min, self.i_max, \n",
    "            device=self.device\n",
    "        ).reshape(-1)\n",
    "\n",
    "        return channel_past, img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24ae3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "629d15c1",
   "metadata": {},
   "source": [
    "# DataLoader 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940a7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 2048]) torch.Size([8, 16, 3, 224, 224]) torch.Size([8, 2048])\n",
      "cuda:0 cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "comm_frames = flatten_comm_frames(dataset.comm_dataset)\n",
    "sensor = dataset.camera_dataset.sensors[\"unit1_cam1\"]\n",
    "\n",
    "ds = MultiModalNextStepDatasetGPU(\n",
    "    comm_frames=comm_frames,\n",
    "    cam_files=sensor.files,\n",
    "    ue_idx=0,\n",
    "    past_len=16,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,     \n",
    "    pin_memory=False   # ✅ 의미 없음 (이미 GPU)\n",
    ")\n",
    "\n",
    "ch, img, y = next(iter(loader))\n",
    "print(ch.shape, img.shape, y.shape)\n",
    "print(ch.device, img.device, y.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2511a",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "data shape 맞추기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c529263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lwm_multi_model import multi_modal_lwm  # 너가 올린 backbone\n",
    "\n",
    "class FinetuneChannelPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    직접 구현한 lwm_multi_model(channel + image)에 맞는 파인튜닝 모델\n",
    "    Input:\n",
    "      ch:  (B, T, F_in)  e.g., (B,16,2048) 16: past Length, 2048: feature dim\n",
    "      img: (B, 3, 224, 224)\n",
    "    Output:\n",
    "      yhat: (B, F_out)  e.g., (B,2048)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        F_in: int,\n",
    "        F_out: int,\n",
    "        pool: str = \"last\",          # \"last\" or \"mean\"\n",
    "        freeze_image: bool = False,\n",
    "        freeze_backbone: bool = False,\n",
    "        element_length: int = 16,    # 채널 벡터 차원 (backbone 기대값)\n",
    "        d_model: int = 64            # backbone 내부 feature dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.pool = pool\n",
    "\n",
    "        # backbone이 기대하는 channel feature dim = ELEMENT_LENGTH\n",
    "        # (backbone 내부 Channel_Embedding: Linear(ELEMENT_LENGTH -> D_MODEL))\n",
    "        if element_length is None:\n",
    "            element_length = backbone.channel_embedding.element_length\n",
    "        if d_model is None:\n",
    "            d_model = backbone.channel_embedding.d_model\n",
    "\n",
    "        # 입력 차원 정렬: F_in -> ELEMENT_LENGTH\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Linear(F_in, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, element_length)\n",
    "        )\n",
    "\n",
    "        # 출력 head: D_MODEL -> F_out\n",
    "        self.head = nn.Linear(d_model, F_out)\n",
    "\n",
    "        if freeze_image:\n",
    "            for p in self.backbone.image_embedding.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "            # 그래도 projection/head는 학습되게 다시 켜기\n",
    "            for p in self.in_proj.parameters():\n",
    "                p.requires_grad = True\n",
    "            for p in self.head.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    def forward(self, ch, img):\n",
    "        # ch: (B,T,F_in) -> (B,T,ELEMENT_LENGTH)\n",
    "        ch = self.in_proj(ch)\n",
    "\n",
    "        # backbone: (B,T,D_MODEL)\n",
    "        tokens = self.backbone(ch, img)\n",
    "\n",
    "        # pooling -> (B,D_MODEL)\n",
    "        if self.pool == \"last\":\n",
    "            z = tokens[:, -1, :]\n",
    "        elif self.pool == \"mean\":\n",
    "            z = tokens.mean(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pool={self.pool}\")\n",
    "\n",
    "        # head -> (B,F_out)\n",
    "        yhat = self.head(z)\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a391b0",
   "metadata": {},
   "source": [
    "## NMSE(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def nmse_db(yhat: torch.Tensor, y: torch.Tensor, eps: float = 1e-16) -> torch.Tensor:\n",
    "    # yhat, y: (B,F)\n",
    "    num = torch.sum((yhat - y) ** 2, dim=1)\n",
    "    den = torch.sum(y ** 2, dim=1).clamp_min(eps)\n",
    "    nmse = num / den\n",
    "    return 10.0 * torch.log10(nmse.clamp_min(eps)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdc594",
   "metadata": {},
   "source": [
    "# Train/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba34747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating min/max over training set...\n",
      "Done. rmin=-1.0646139668415024e-06, rmax=1.091798991441695e-06, imin=-1.0719515918791155e-06, imax=1.0643867764302909e-06\n",
      "Dataset statistical values set in the dataset.\n",
      "\n",
      "=== Data Check ===\n",
      "y stats | min: 0.0005, max: 1.0000\n",
      "If scaling worked correctly, values should be within [0, 1].\n",
      "====\n",
      "check the type float 32 or float 64?\n",
      "ch dtype: torch.float64 y dtype: torch.float64\n"
     ]
    }
   ],
   "source": [
    "n = len(ds)\n",
    "n_train = int(0.75 * n)\n",
    "train_idx = list(range(0, n_train))\n",
    "val_idx = list(range(n_train, n))\n",
    "\n",
    "train_ts = [ds.valid_start + i for i in train_idx]\n",
    "\n",
    "(real_min,  real_max), (imag_min, imag_max) = get_train_min_max_realimag(\n",
    "    comm_frames, train_ts, us_idx=0\n",
    ")\n",
    "\n",
    "ds.r_min = real_min\n",
    "ds.r_max = real_max\n",
    "ds.i_min = imag_min\n",
    "ds.i_max = imag_max\n",
    "\n",
    "print(\"Dataset statistical values set in the dataset.\")\n",
    "\n",
    "train_ds = Subset(ds, train_idx)\n",
    "val_ds   = Subset(ds, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Verify\n",
    "ch, img, y = next(iter(train_loader))\n",
    "print(\"\\n=== Data Check ===\")\n",
    "print(f\"y stats | min: {y.min().item():.4f}, max: {y.max().item():.4f}\")\n",
    "print(\"If scaling worked correctly, values should be within [0, 1].\")\n",
    "\n",
    "print(\"====\")\n",
    "print(\"check the type float 32 or float 64?\")\n",
    "print(\"ch dtype:\", ch.dtype, \"y dtype:\", y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdd060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(comm_frames): 100\n",
      "len(cam_files): 7012\n",
      "first comm frame keys: ['bs_loc', 'ue', 'ue_loc', 'bs']\n",
      "first cam file: scenarios/DT31/RGB_images/unit1_cam1/0.png\n"
     ]
    }
   ],
   "source": [
    "comm_frames = flatten_comm_frames(dataset.comm_dataset)\n",
    "cam_files = list(dataset.camera_dataset.sensors[\"unit1_cam1\"].files)\n",
    "\n",
    "print(\"len(comm_frames):\", len(comm_frames))\n",
    "print(\"len(cam_files):\", len(cam_files))\n",
    "print(\"first comm frame keys:\", list(comm_frames[0].keys()))\n",
    "print(\"first cam file:\", cam_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa662c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbf8fb",
   "metadata": {},
   "source": [
    "# Model generate and also check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692cc0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Detected: F_in= 2048 F_out= 2048\n",
      "Batch devices: cuda:0 cuda:0 cuda:0\n",
      "yhat: torch.Size([32, 2048]) y: torch.Size([32, 2048])\n"
     ]
    }
   ],
   "source": [
    "# 배치 하나로 F_in/F_out 자동 확정\n",
    "ch, img, y = next(iter(train_loader))\n",
    "F_in  = ch.shape[-1]\n",
    "F_out = y.shape[-1]\n",
    "print(\"Detected:\", \"F_in=\", F_in, \"F_out=\", F_out)\n",
    "print(\"Batch devices:\", ch.device, img.device, y.device)\n",
    "\n",
    "# backbone + finetune model\n",
    "backbone = multi_modal_lwm().to(device)\n",
    "\n",
    "model = FinetuneChannelPredictor(\n",
    "    backbone=backbone,\n",
    "    F_in=F_in,\n",
    "    F_out=F_out,\n",
    "    pool=\"last\",            # \"mean\"으로 바꿔도 됨\n",
    "    freeze_image=False,     # 원하면 True (이미지 인코더 고정)\n",
    "    freeze_backbone=False,  # 원하면 True (proj/head만 학습)\n",
    "    element_length=16,\n",
    "    d_model=64\n",
    ").to(device).double()\n",
    "\n",
    "# sanity forward\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # ds가 이미 cuda 텐서 반환이면 아래 .to(device) 생략 가능\n",
    "    yhat = model(ch.to(device), img.to(device))\n",
    "print(\"yhat:\", yhat.shape, \"y:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958be0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Type: torch.float64\n",
      "Model weight Type: torch.float64\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 생성 (백본 포함)\n",
    "model = FinetuneChannelPredictor(\n",
    "    backbone=backbone,\n",
    "    F_in=F_in,\n",
    "    F_out=F_out,\n",
    "    element_length=16,\n",
    "    d_model=64\n",
    ").to(device)\n",
    "\n",
    "# 2. 전체 모델을 Double(float64)로 강제 변환\n",
    "# 이 한 줄이 실행되어야 'in_proj'와 'head' 가중치가 모두 float64가 됩니다.\n",
    "model.double() \n",
    "\n",
    "# 3. 타입 불일치 확인 (디버깅용)\n",
    "print(f\"Input Type: {ch.dtype}\")                # torch.float64 여야 함\n",
    "print(f\"Model weight Type: {model.head.weight.dtype}\") # torch.float64 여야 함\n",
    "\n",
    "# 4. 연산 수행\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 데이터와 모델 가중치 타입을 일치시켜서 입력\n",
    "    # .to(device) 뒤에 .double()을 붙여서 확실히 합니다.\n",
    "    yhat = model(ch.to(device).double(), img.to(device).double())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5893214",
   "metadata": {},
   "source": [
    "# Train/ Eval 함수 (grad clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_nmse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for ch, img, y in loader:\n",
    "        # Dataset이 이미 cuda 텐서를 반환하더라도 안전하게 유지\n",
    "        ch = ch.to(device, non_blocking=True).double()\n",
    "        img = img.to(device, non_blocking=True).double()\n",
    "        y  = y.to(device, non_blocking=True).double()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        yhat = model(ch, img)\n",
    "        loss = F.mse_loss(yhat, y)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip is not None and grad_clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_nmse += nmse_db(yhat.detach(), y).item()\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(n, 1), total_nmse / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_nmse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for ch, img, y in loader:\n",
    "        ch = ch.to(device, non_blocking=True).double()\n",
    "        img = img.to(device, non_blocking=True).double()\n",
    "        y  = y.to(device, non_blocking=True).double()\n",
    "\n",
    "        yhat = model(ch, img)\n",
    "        loss = F.mse_loss(yhat, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_nmse += nmse_db(yhat, y).item()\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(n, 1), total_nmse / max(n, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ab5d4",
   "metadata": {},
   "source": [
    "#  Optiimizer  / Scheduler 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2499120\n"
     ]
    }
   ],
   "source": [
    "# requires_grad=True인 파라미터만 학습\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"trainable params:\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# (선택) cosine scheduler\n",
    "epochs = 600\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897236b",
   "metadata": {},
   "source": [
    "# 학습 루프 + checkpoint 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758db97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/600] train loss=0.590881, nmse(dB)=3.1930 | val loss=0.479787, nmse(dB)=2.6894 | 89.9s\n",
      "  ↳ saved best_finetune.pt | best@1: val loss=0.479787, val nmse(dB)=2.6894\n",
      "[02/600] train loss=0.521118, nmse(dB)=2.6463 | val loss=0.441064, nmse(dB)=2.3238 | 93.6s\n",
      "  ↳ saved best_finetune.pt | best@2: val loss=0.441064, val nmse(dB)=2.3238\n",
      "[03/600] train loss=0.489506, nmse(dB)=2.3735 | val loss=0.416097, nmse(dB)=2.0706 | 95.8s\n",
      "  ↳ saved best_finetune.pt | best@3: val loss=0.416097, val nmse(dB)=2.0706\n",
      "[04/600] train loss=0.466481, nmse(dB)=2.1613 | val loss=0.394099, nmse(dB)=1.8346 | 96.8s\n",
      "  ↳ saved best_finetune.pt | best@4: val loss=0.394099, val nmse(dB)=1.8346\n",
      "[05/600] train loss=0.444737, nmse(dB)=1.9532 | val loss=0.375637, nmse(dB)=1.6261 | 92.6s\n",
      "  ↳ saved best_finetune.pt | best@5: val loss=0.375637, val nmse(dB)=1.6261\n",
      "[06/600] train loss=0.424612, nmse(dB)=1.7507 | val loss=0.362005, nmse(dB)=1.4655 | 168.8s\n",
      "  ↳ saved best_finetune.pt | best@6: val loss=0.362005, val nmse(dB)=1.4655\n",
      "[07/600] train loss=0.413882, nmse(dB)=1.6361 | val loss=0.349863, nmse(dB)=1.3172 | 90.6s\n",
      "  ↳ saved best_finetune.pt | best@7: val loss=0.349863, val nmse(dB)=1.3172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      8\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m     tr_loss, tr_nmse \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, optimizer, device\u001b[38;5;241m=\u001b[39mdevice, grad_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     11\u001b[0m     va_loss, va_nmse \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, device, grad_clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m total_nmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      6\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch, img, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Dataset이 이미 cuda 텐서를 반환하더라도 안전하게 유지\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     ch \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[1;32m     11\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdouble()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m, in \u001b[0;36mMultiModalNextStepDatasetGPU.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     36\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcam_files[k]\n\u001b[0;32m---> 37\u001b[0m     img_k \u001b[38;5;241m=\u001b[39m preprocess_img(img_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m     img_list\u001b[38;5;241m.\u001b[39mappend(img_k)\n\u001b[1;32m     39\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(img_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (past_len\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mpreprocess_img\u001b[0;34m(path, img_size, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2) numpy -> torch, (3,H,W), float32\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(arr)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mdouble()  \u001b[38;5;66;03m# (3,H,W), double for ImageNet stats\u001b[39;00m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# [0,1]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 3) add batch dim -> (1,3,H,W)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val = float(\"inf\")\n",
    "best_val_nmse = None\n",
    "best_epoch = None\n",
    "\n",
    "t_train0 = time.time()  # ✅ 전체 학습 시작 시간\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_nmse = train_one_epoch(model, train_loader, optimizer, device=device, grad_clip=1.0)\n",
    "    va_loss, va_nmse = evaluate(model, val_loader, device=device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(\n",
    "        f\"[{epoch:02d}/{epochs}] \"\n",
    "        f\"train loss={tr_loss:.6f}, nmse(dB)={tr_nmse:.4f} | \"\n",
    "        f\"val loss={va_loss:.6f}, nmse(dB)={va_nmse:.4f} | \"\n",
    "        f\"{dt:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_val_nmse = va_nmse\n",
    "        best_epoch = epoch\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"F_in\": F_in,\n",
    "                \"F_out\": F_out,\n",
    "                \"best_val_loss\": best_val,\n",
    "                \"best_val_nmse_db\": best_val_nmse,\n",
    "            },\n",
    "            \"best_finetune.pt\"\n",
    "        )\n",
    "        print(f\"  ↳ saved best_finetune.pt | best@{best_epoch}: val loss={best_val:.6f}, val nmse(dB)={best_val_nmse:.4f}\")\n",
    "\n",
    "# ✅ 전체 학습 종료 후 총 시간 출력\n",
    "total_sec = time.time() - t_train0\n",
    "h = int(total_sec // 3600)\n",
    "m = int((total_sec % 3600) // 60)\n",
    "s = total_sec % 60\n",
    "\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Total time: {total_sec:.1f}s ({h}h {m}m {s:.1f}s)\")\n",
    "print(f\"Best epoch: {best_epoch} | best val loss={best_val:.6f}, best val nmse(dB)={best_val_nmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23882e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y abs mean: 0.49508270621299744\n",
      "y abs max : 1.0\n",
      "y power   : 0.2848026752471924\n",
      "yhat abs mean: 0.495068222284317\n",
      "yhat abs max : 0.5914323925971985\n",
      "yhat power   : 0.24561259150505066\n"
     ]
    }
   ],
   "source": [
    "ch, img, y = next(iter(train_loader))\n",
    "print(\"y abs mean:\", y.abs().mean().item())\n",
    "print(\"y abs max :\", y.abs().max().item())\n",
    "print(\"y power   :\", (y**2).mean().item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat = model(ch.to(device), img.to(device))\n",
    "print(\"yhat abs mean:\", yhat.abs().mean().item())\n",
    "print(\"yhat abs max :\", yhat.abs().max().item())\n",
    "print(\"yhat power   :\", (yhat**2).mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a090a4",
   "metadata": {},
   "source": [
    "# 데이터 입력 및 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fcab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== dataset sizes ===\n",
      "N(comm_frames): 100\n",
      "N(cam_files)  : 7012\n",
      "N(min)        : 100\n",
      "past_len      : 16\n",
      "len(ds)       : 84\n",
      "len(train_ds) : 63\n",
      "len(val_ds)   : 21\n",
      "len(train_loader): 2\n",
      "len(val_loader)  : 1\n",
      "\n",
      "=== one batch shapes ===\n",
      "ch : (32, 16, 2048)  -> (B,T,F_in)\n",
      "img: (32, 16, 3, 224, 224)  -> (B,3,224,224)\n",
      "y  : (32, 2048)  -> (B,F_out)\n",
      "yhat: (32, 2048)  -> (B,F_out)\n",
      "this forward predicted vectors: 32 (=B)\n",
      "each vector predicts elements: 2048 (=F_out)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== dataset sizes ===\")\n",
    "print(\"N(comm_frames):\", len(comm_frames))\n",
    "print(\"N(cam_files)  :\", len(cam_files))\n",
    "print(\"N(min)        :\", min(len(comm_frames), len(cam_files)))\n",
    "print(\"past_len      :\", ds.past_len)\n",
    "print(\"len(ds)       :\", len(ds))\n",
    "print(\"len(train_ds) :\", len(train_ds))\n",
    "print(\"len(val_ds)   :\", len(val_ds))\n",
    "print(\"len(train_loader):\", len(train_loader))\n",
    "print(\"len(val_loader)  :\", len(val_loader))\n",
    "\n",
    "print(\"\\n=== one batch shapes ===\")\n",
    "ch, img, y = next(iter(train_loader))\n",
    "print(\"ch :\", tuple(ch.shape), \" -> (B,T,F_in)\")\n",
    "print(\"img:\", tuple(img.shape), \" -> (B,3,224,224)\")\n",
    "print(\"y  :\", tuple(y.shape), \" -> (B,F_out)\")\n",
    "with torch.no_grad():\n",
    "    yhat = model(ch.to(device), img.to(device))\n",
    "print(\"yhat:\", tuple(yhat.shape), \" -> (B,F_out)\")\n",
    "print(\"this forward predicted vectors:\", yhat.shape[0], \"(=B)\")\n",
    "print(\"each vector predicts elements:\", yhat.shape[1], \"(=F_out)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792a69d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ffde29c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoyun_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
